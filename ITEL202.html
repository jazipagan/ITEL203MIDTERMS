<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ITEL202</title>
    <link rel="website icon" type="jpg" href="icon.jpg">
    <link rel="stylesheet" href="ITEL202.css">
</head>
<body>
  <!--Header-->
  <div id="header">
    <div id="mySidenav" class="sidenav">
      <a href="javascript:void(0)" class="closebtn" onclick="closeNav()">&times;</a>
      <a href="Homepage.html">Home</a>
      <a href="home.html">Reviewer</a>
      <a href="About.html">About Us</a>
      <a href="Contact.html">Contact</a>
    </div>
    <span onclick="openNav()">&#9776; ITEL202</span>
    <script src="Navigation.js"></script>
</div>
  <!--Contents-->
  <div id = "content">
    <h1 style="text-align: center;">Platform Technologies</h1>
    <div id = "P1ITEL202">
        <h1 style="text-align: center;"> <b>Part 1</b></h1>
        <p>
            <b>THE MAIN COMPONENTS OF A COMPUTER</b><br>
                Although it is difficult to distinguish between the ideas belonging to computer organization and those ideas belonging to computer architecture, it is impossible to say where hardware issues end and software issues begin. Computer scientists design algorithms that usually are implemented as programs written in some computer language, such as Java or C++. But what makes the algorithm run? Another algorithm, of course! And another algorithm runs that algorithm, and so on until you get down to the machine level, which can be thought of as an algorithm implemented as an electronic device.<br>
                • Principle of Equivalence of Hardware and Software: Any task done by software can also be done using hardware, and any operation performed directly by hardware can be done using software.<br>
                A special-purpose computer can be designed to perform any task, such as word processing, budget analysis, or playing a friendly game of Tetris.<br>
                Equivalence of Hardware and Software tells us that we have a choice. Our knowledge of computer organization and architecture will help us to make the best choice.<br>
                We begin our discussion of computer hardware by looking at the components necessary to build a computing system. At the most basic level, a computer is a device consisting of three pieces:<br><br>
                1. A processor to interpret and execute programs.<br>
                2. A memory to store both data and programs.<br>
                3. A mechanism for transferring data to and from the outside world.<br><br>
                <b>Tablet Computers</b><br>
                A touchscreen dominates the real estate of all portable devices. For consumer tablets and phones, touchscreens come in two general types: resistive and capacitive. Resistive touchscreens respond to the pressure of a finger or a stylus. Capacitive touchscreens react to the electrical properties of the human skin. Resistive screens are less sensitive than capacitive screens, but they provide higher resolution. Unlike resistive screens, capacitive screens support multi-touch, which is the ability to detect the simultaneous press of two or more fingers.<br><br>
                <b>STANDARDS ORGANIZATIONS</b><br>
                Suppose you decide you'd like to have one of those nifty new LCD widescreen monitors. You figure you can shop around a bit to find the best price. You make a few phone calls, surf the Web, and drive around town until you find the one that gives you the most for your money. From your experience, you know you can buy your monitor anywhere and it will probably work fine on your system.<br>
                Some of these standards-setting organizations are ad hoc trade associations or consortia made up of industry leaders.<br>
                As you continue your studies in computer organization and architecture, you will encounter specifications formulated by these groups, so you should know something about them.<br>
                • Institute of Electrical and Electronics Engineers (IEEE) is an organization dedicated to the advancement of the professions of electronic and computer engineering. The IEEE actively promotes the interests of the worldwide engineering community by publishing an array of technical literature.<br>
                • International Telecommunications Union (ITU) is based in Geneva, Switzerland. The ITU was formerly known as the Comité Consultatif International Télégraphique et Téléphonique, or the International Consultative Committee on Telephony and Telegraphy. As its name implies, the ITU concerns itself with the interoperability of telecommunications systems, including telephone, telegraph, and data communication systems.<br>
                • International Organization for Standardization (ISO) is the entity that coordinates worldwide standards development, including the activities of ANSI with BSI, among others. ISO is not an acronym but derives from the Greek word, isos, meaning “equal.” The ISO consists of more than 2800 technical committees, each of which is charged with some global standardization issue. Its interests range from the behavior of photographic film to the pitch of screw threads to the complex world of computer engineering. The proliferation of global trade has been facilitated by the ISO. Today, the ISO touches virtually every aspect of our lives.<br><br>
                <b>HISTORICAL DEVELOPMENT</b><br>
                During their 60-year life span, computers have become the perfect example of modern convenience. Living memory is strained to recall the days of steno pools, carbon paper, and mimeograph machines. It sometimes seems that these magical computing machines were developed instantaneously in the form that we now know them.<br>
                • Occasionally computers have even improved through the application of solid engineering practices! Despite all the twists, turns, and technological dead ends, computers have evolved at a pace that defies comprehension.<br>
                • Has the word computer now become a misnomer? An anachronism? What, then, should we call them, if not computers? We cannot present the complete history of computing in a few pages.<br>
                • No longer limited to white-jacketed scientists, today’s computers help us to write documents, keep in touch with loved ones across the globe, and do our shopping chores.<br>
                • How much computation do we actually see pouring from the mysterious boxes perched on or beside our desks? Until recently, computers served us only by performing mind-bending mathematical manipulations<br><br>
                <b>Generation Zero: Mechanical Calculating Machines (1642– 1945)</b><br>
                Ada expressed her delight with this idea, writing, “[T]he Analytical Engine weaves algebraically patterns just as the Jacquard loom weaves flowers and leaves.” The punched card proved to be the most enduring means of providing input to a computer system.<br>
                The Analytical Engine included many of the components associated with modern computers: an arithmetic processing unit to perform calculations (Babbage referred to this as the mill), a memory (the store), and input and output devices.<br>
                Babbage designed the Analytical Engine to use a type of punched card for input and programming.<br>
                Babbage also designed a general-purpose machine in 1833 called the Analytical Engine.<br><br>
                <b>The First Generation: Vacuum Tube Computers (1945–1953)</b><br>
                They portrayed their machine as conservatively as they could, billing it as an “automatic calculator.” Although they probably knew that computers would be able to function most efficiently using the binary numbering system, Mauchly and Eckert designed their system to use base 10 numbers, in keeping with the appearance of building a huge electronic adding machine.<br>
                Pursuant to the Allied war effort, and with ulterior motives to learn about electronic computation, Mauchly volunteered for a crash course in electrical engineering at the University of Pennsylvania’s Moore School of Engineering.<br>
                The Atanasoff Berry Computer (ABC) was a binary machine built from vacuum tubes.<br>
                John Mauchly’s vision for an electronic calculating machine was born from his lifelong interest in predicting the weather mathematically. <br><br>
                <b>What Is a Vacuum Tube?</b><br>
                Vacuum tubes should be called valves because they control the flow of electrons in electrical systems in much the same way as valves control the flow of water in a plumbing system.<br>
                He knew that thermionic emission supported the flow of electrons in only one direction: from the negatively charged cathode to the positively charged anode, also called a plate.<br>
                The control grid, when carrying a negative charge, can reduce or prevent electron flow from the cathode to the anode of a diode.<br><br>
                <b>The Second Generation: Transistorized Computers (1954–1965)</b><br>
                Nevertheless, a plethora of computer makers emerged in this generation; IBM, Digital Equipment Corporation (DEC), and Univac (now Unisys) dominated the industry.<br>
                Because transistors consume less power than vacuum tubes, are smaller, and work more reliably, the circuitry in computers consequently became smaller and more reliable.<br><br>
                <b>What Is a Transistor?</b><br>
                The resulting peaks and valleys of P- and N-type material form microscopic electronic components, including transistors, that behave just like larger versions fashioned from discrete components, except that they run a lot faster and consume a small fraction of the power.<br>
                So if you add a small amount of aluminum to silicon, the silicon ends up with a slight imbalance in its outer electron shell, and therefore attracts electrons from any pole that has a negative potential (an excess of electrons).<br>
                If the poles are reversed, that is, if we apply a negative potential to the N-type material and a positive potential to the P-type material, no current will flow.<br>
                In other words, if we apply a positive potential to N-type material, electrons will flow from the negative pole to the positive pole.<br>
                The base in a transistor works just like the control grid in a triode tube: Small changes in the current at the base of a transistor result in a large electron flow from the emitter to the collector.<br><br>
                <b>The Third Generation: Integrated Circuit Computers (1965– 1980)</b><br>
                Six months later, Robert Noyce (who had also been working on integrated circuit design) created a similar device using silicon instead of germanium.<br>
                Early ICs allowed dozens of transistors to exist on a single silicon chip that was smaller than a single “discrete component” transistor.<br>
                The IBM System/360 family of computers was among the first commercially available systems to be built entirely of solid-state components.<br><br>
                <b>The Fourth Generation: VLSI Computers (1980–????)</b><br>
                Comparison of Computer Components Clockwise, starting from the top: <br>
                1) Vacuum tube<br>
                2) Transistor <br>
                3) Chip containing 3200 2-input NAND gates <br>
                4) Integrated circuit package (the small silver square in the lower left-hand corner is an integrated circuit) Courtesy of Linda Null.<br>
                There are now various levels of integration: SSI (small-scale integration), in which there are 10 to 100 components per chip; MSI (medium-scale integration), in which there are 100 to 1000 components per chip; LSI (large-scale integration), in which there are 1000 to 10,000 components per chip; and finally, VLSI (very-large-scale integration), in which there are more than 10,000 components per chip.<br>
                In the third generation of electronic evolution, multiple transistors were integrated onto one chip.<br>
                Other useful terminology includes: <br>
                (1) WSI (wafer-scale integration, building superchip ICs from an entire silicon wafer;<br>
                (2) 3D-IC (three-dimensional integrated circuit); and <br>
                (3) SOC (system-on-a-chip), an IC that includes all the necessary components for the entire computer.<br><br>
                <b>Moore’s Law</b><br>
                Rock’s Law, proposed by early Intel capitalist Arthur Rock, is a corollary to Moore’s Law: “The cost of capital equipment to build semiconductors will double every four years.” Rock’s Law arises from the observations of a financier who saw the price tag of new chip facilities escalate from about $12,000 in 1968 to $12 million in the mid-1990s.<br><br>
                <b>THE COMPUTER LEVEL HIERARCHY</b><br><br>
                <b>Levels of Computing as a Service</b><br>
                Cloud computing services can be defined and delivered in a number of ways based on levels of the computer hierarchy shown again in Figure 1.4. At the top of the hierarchy, where we have executable programs, a Cloud provider might offer an entire application over the Internet, with no components installed locally.<br>
                Well-known PaaS providers include Google App Engine and Microsoft Windows Azure Cloud Services [as well as Force.com (PaaS provided by Salesforce.com)]. PaaS is not a good fit in situations where rapid configuration changes are required.<br>
                Indeed, in any company where staff is capable of managing operating system and database software, the Infrastructure as a Service (IaaS) Cloud model might be the best option.<br>
                The general public can obtain small amounts of Cloud storage inexpensively through services such as Dropbox, Google Drive, and Amazon.com’s Cloud Drive —to name only a few among a crowded field.<br><br>
                <b>THE VON NEUMANN MODEL</b><br>
                Today’s version of the stored-program machine architecture satisfies at least the following characteristics:<br>
                • Consists of three hardware systems: A central processing unit (CPU) with a control unit, an arithmetic logic unit (ALU), registers (small storage areas), and a program counter; a main memory system, which holds programs that control the computer’s operation; and an I/O system.<br>
                This architecture runs programs in what is known as the von Neumann execution cycle (also called the fetch-decode-execute cycle), which describes how the machine works The ideas present in the von Neumann architecture have been extended so that programs and data stored in a slow-to-access storage medium, such as a hard disk, can be copied to a fast-access, volatile storage medium such as RAM prior to execution.<br>
                This architecture has also been streamlined into what is currently called the system bus model, which is shown in Figure 1.6. The data bus moves data from main memory to the CPU registers (and vice versa).<br>
                Contains a single path, either physically or logically, between the main memory system and the control unit of the CPU, forcing alternation of instruction and execution cycles.<br>
                fetch-decode-execute cycle: One iteration of the cycle is as follows<br>
                • The control unit fetches the next program instruction from the memory, using the program counter to determine where the instruction is located. <br>
                • The instruction is decoded into a language the ALU can understand. <br>
                • Any data operands required to execute the instruction are fetched from memory and placed in registers in the CPU. <br>
                • The ALU executes the instruction and places the results in registers or memory<br><br>
                <b>The Modified von Neumann Architecture, Adding a System Bus</b><br>
                Other enhancements to the von Neumann architecture include using index registers for addressing, adding floating-point data, using interrupts and asynchronous I/O, adding virtual memory, and adding general registers<br><br>

                <b>NON–VON NEUMANN MODELS</b><br>
                A number of different subfields fall into the non–von Neumann category, including neural networks (using ideas from models of the brain as a computing paradigm) implemented in silicon, cellular automata, cognitive computers (machines that learn by experience rather than through programming, including IBM’s SyNAPSE computer, a machine that models the human brain), quantum computation (a combination of computing and quantum physics), dataflow computation, and parallel computers.<br>
                Other non–von Neumann computers include digital signal processors (DSPs) and media processors, which can execute a single instruction on a set of data (instead of executing a single instruction on a single piece of data).<br>
                For example, an architecture that does not store programs and data in memory or does not process a program sequentially would be considered a non–von Neumann machine.<br><br>

                <b>PARALLEL PROCESSORS AND PARALLEL COMPUTING</b><br>
                To summarize, parallel processing refers to a collection of different architectures, from multiple separate computers working together, to multiple processors sharing memory, to multiple cores integrated onto the same chip.<br>
                In this regard, perhaps it is more appropriate to say that parallel processing exhibits “non–von Neumannness.” Regardless of how parallel processors are classified, parallel computing allows us to multitask and to solve larger and more complex problems, and is driving new research in various software tools and programming.<br>
                However, many argue that parallel processing computers contain CPUs, use program counters, and store both programs and data in main memory, which makes them more like an extension to the von Neumann architecture rather than a departure from it; these people view parallel processing computers as sets of cooperating von Neumann machines.<br>
                But what is a core? Instead of a single processing unit in an integrated circuit (as found in typical von Neumann machines), independent multiple cores are “plugged in” and run in parallel.<br><br>

                <b>PARALLELISM: ENABLER OF MACHINE INTELLIGENCE —DEEP BLUE AND WATSON</b><br>
                Watson’s designers, therefore, approached the situation just as a human being would: Watson “learned” by consuming terabytes of unstructured data from thousands of news sources, journals, and books.<br>
                Watson was then given 25,000 test case scenarios and 1500 real-life cases from which it demonstrated that it had gained the ability to derive meaning from the mountain of complex medical data, some of which was in informal natural language—such as doctors’ notes, patient records, medical annotations, and clinical feedback.<br>
                It is evident by our sidebar on the Mechanical Turk that chess playing has long been considered the ultimate demonstration of a “thinking machine.” The chess-board is a battlefield where human can meet machine on more-or-less equal terms—with the human always having the edge, of course.<br>
                Beginning in 2011, IBM, WellPoint, and Memorial Sloan-Kettering Cancer Center set Watson to work absorbing more than 600,000 pieces of medical evidence, and two million pages of text from 42 medical journals and oncology research documents.<br><br>

            </p>
    </div>
    <div id = "P2ITEL202">
        <h1 style="text-align: center;"> <b>Part 2</b></h1>
        <p>
            <b>INTRODUCTION</b><br>
            • The organization of any computer depends considerably on how it represents numbers, characters, and control information. The converse is also true: Standards and conventions established over the years have determined certain aspects of computer organization. This chapter describes the various ways in which computers can store and manipulate numbers and characters. The ideas presented in the following sections form the basis for understanding the organization and function of all types of digital systems.<br>
            • The most basic unit of information in a digital computer is called a bit, which is a contraction of binary digit. In the concrete sense, a bit is nothing more than a state of “on” or “off” (or “high” and “low”) within a computer circuit. In 1964, the designers of the IBM System/360 mainframe computer established a convention of using groups of 8 bits as the basic unit of addressable computer storage. They called this collection of 8 bits a byte.<br>
            • Computer words consist of two or more adjacent bytes that are sometimes addressed and almost always are manipulated collectively. The word size represents the data size that is handled most efficiently by a particular architecture. Words can be 16 bits, 32 bits, 64 bits, or any other size that makes sense in the context of a computer’s organization (including sizes that are not multiples of eight). An 8-bit byte can be divided into two 4-bit halves called nibbles (or nybbles). Because each bit of a byte has a value within a positional numbering system, the nibble containing the least-valued binary digit is called the low-order nibble, and the other half the high-order nibble.<br><br>

            <b>The computer number system</b><br>
            <b>Why we need to study the computer number system?</b><br>
            • Computer and networking equipments work with binary digits (bits)<br>
            • In the other words, the 2 numbers system (binary numbers system) is what the  computers and data communication devices are using for its designs.<br>
            • Bits can be eithers a binary 1 or binary 0 that can represent as the absence (0) or presence (1) of current which flows within a cable wire or circuitry in the computers system<br>
            • In switching system application , 1 can be an ON state, while 0 can be an OFF State.<br>
            • In writing the PROGRAMMING LOGICAL or ALGORITHM, 1 can be Interpreted as true or yes, while 0 can be interpreted as false or no.<br>
            • Internet Protocol (IP) addresses are usually written as dotted-decimal numbers separated by period (dots) each  representing an octet so that we can read them easily.<br>
            • Knowing and learning binary numbers and and how they relate to decimal and hexadecimal numbers are critical to understanding successfully the  network routing, IP addresses ,subnets, and computer circuitry.<br><br>

            <b>Presenting the binary numbers systems</b><br>
            <b>Decimal numbers system</b><br>
            • A decimal number can be expressed as the sum of each digit times a power of ten expanded notation. With decimal fraction, this can be expressed also in expanded notation. <br>
            • However the value at the right side of the decimal point are the negative power of ten<br><br>
            Jerico
            Jerico Silva
            <b>Binary to decimal numbers conversion</b><br>
            • Binary numbers can be converted into decimal numbers using an expanded notation in base 2 instead of base 10 (in the case of decimal numbers).<br>
            • Example:<br>
            1). 102 = 1 x 2<sup>1</sup> + 0 x 2<sup>0</sup><br>
            = 2 + 0<br>
            = 2<sub>10</sub><br><br>
            2). 1102 = 1 x 2<sup>2</sup> + 1 x 2<sup>1</sup> + 0 x 2<sup>0</sup><br>
            = 4 + 2 + 0<br>
            = 6<sub>10</sub><br><br>
            3). 11112 = 1 x 2<sup>3</sup> + 1 x 2<sup>2</sup> + 1 x 2<sup>1</sup> + 1 x 2<sup>0</sup><br>
            = 8 + 4 + 2 + 1<br>
            = 15<sub>10</sub><br><br>

            <b>Decimal to binary numbers conversion</b><br>
            • Decimal numbers can be converted into binary numbers by dividing it by 2. The remainder is considered as its binary equivalent by reading it upward or the last remainder is the first to be read. You have to neglect the numbers after the decimal point in the quotient.<br>
            Example 1.<br>
            Divide<br>
            1. Divide by 2 Result Binary Value<br>
            73 ÷ 2 36 1<br>
            36 ÷ 2 18 0<br>
            18 ÷ 2 9 0<br>
            9 ÷ 2 4 1<br><br>

            The octal number system<br>
            • The Octal Number System has eight basic digits: 0,1,2,3,4,5,6,7.<br>
            • It is a base 8 number system.<br>
            • It is used to conserve memory storage location of the computer system by grouping the binary digits into three. Meaning, 3 bits is equivalent to 1 octal number.<br>

            <b>Decimal to octal number conversion</b><br>
            • To convert a decimal number into octal number, first we divide the decimal number by 8. Then we have to take note of the remainder after each computation of division operation. The computation process will stop when the quotient becomes 0. Again, we have to read the remainders in an upward direction or the last number is to be read first.<br>
            Example 1<br>
            Example 2<br><br>

            <b>Octal to decimal number conversion</b><br>
            • To convert an octal number to a decimal number, we have to multiply each octal number by its positional value. Then we sum up all the resulting products.<br>
            Example 1<br>
            1) 148 = 1 x 8<sup>1</sup> + 4 x 8<sup>0</sup><br>
            = 8 + 4<br>
            = 12<sub>10</sub><br>
            Example 2<br>
            2) 2308 = 2 x 8<sup>2</sup> + 3 x 8<sup>1</sup> + 0 x 8<sup>0</sup><br>
            = 128 + 24 + 0<br>
            = 152<sub>10</sub><br><br>

            <b>OCTAL TO BINARY NUMBER CONVERSION</b><br>
            • Using the given table below, we can convert an octal number to its equivalent binary number. Each octal number must be converted one at a time by its equivalent binary number.<br>
            CONVERSION TABLE<br>
            Example 1<br>
            1) 368 = CONVERT 3 → 011<sub>2</sub><br>
                CONVERT 6 → 110<sub>2</sub><br>
                = 011110<sub>2</sub><br>
            Example 2<br>
            2) 1428 = CONVERT 1 → 001<sub>2</sub><br>
                CONVERT 4 → 100<sub>2</sub><br>
                CONVERT 2 → 010<sub>2</sub><br>
                = 001100010<sub>2</sub><br>
            Example 2<br>
            3) 75.038 = CONVERT 7 → 111<sub>2</sub><br>
                    CONVERT 5 → 101<sub>2</sub><br>
                    CONVERT 0 → 000<sub>2</sub><br>
                    CONVERT 3 → 011<sub>2</sub><br>
                    = 111101.000011<sub>2</sub><br><br>

            <b>OTHER SOLUTION</b><br>
            • We can solve this octal to binary conversion by using the following technique:<br>
            • To be able to convert 78 in binary, we turn ON (or set to 1) the numbers that will sum up to 7.<br><br>

            <b>Hexadecimal System</b><br>
            • Hexadecimal System uses 16 digits: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, F<br>
            And thus the base is 16.<br>
            • Hexadecimal numbers are compact and easy to read.<br>
            • It is very easy to convert numbers from the binary system to the hexadecimal system and vice-versa, every nibble (4 bits) can be converted to a hexadecimal digit using this table:<br><br>

            <b>Hexadecimal System cont.</b><br>
            • There is a convention to add "h" in the end of a hexadecimal number, this way we can determine that 5Fh is a hexadecimal number with a decimal value of 95.<br>
            We also add "0" (zero) in the beginning of hexadecimal numbers that begin with a letter (A..F), for example 0E120h.<br><br>

            The hexadecimal number 1234h is equal to the decimal value of 4660:<br><br>

            <b>Converting from Decimal System to Any Other</b><br>
            In order to convert from the decimal system to any other system, it is required to divide the decimal value by the base of the desired system, each time you should remember the result and keep the remainder, the division process continues until the result is zero.<br>
            The remainders are then used to represent a value in that system.<br>
            Let's convert the value of 39 (base 10) to Hexadecimal System (base 16):<br>
            39/16 = 2 remainder 7<br>
            2/16 = 0 remainder 2<br>
            stop then read the remainder upward<br>
            As you see we got this hexadecimal number: 27 <br>
            All remainders were below 10 in the above example, so we do not use any letters<br><br>
            <b>Signed Numbers</b><br>
            There is no way to say for sure whether the hexadecimal byte 0FFh is positive or negative; it can represent both decimal values "255" and "-1".<br><br>

            8 bits can be used to create 256 combinations (including zero), so we simply presume that the first 128 combinations (0..127) will represent positive numbers and the next 128 combinations (128..255) will represent negative numbers.<br><br>

            In order to get "-5", we should subtract 5 from the number of combinations (256), so we'll get: 256 - 5 = 251.<br><br>

            Using this complex way to represent negative numbers has some meaning; in math, when you add "-5" to "5" you should get zero. This is what happens when a processor adds two bytes, 5 and 251; the result gets over 255, and because of the overflow, the processor gets zero!<br><br>

            When combinations 128..255 are used, the high bit is always 1, so this may be used to determine the sign of a number.<br><br>

            <b>Terms to Remember:</b><br>
            The same principle is used for words (16-bit values). 16 bits create 65,536 combinations; the first 32,768 combinations (0..32767) are used to represent positive numbers, and the next 32,768 combinations (32768..65535) represent negative numbers.<br><br>
        </p>
    </div>
    <div id = "P3ITEL202">
        <h1 style="text-align: center;"> <b>Part 3</b></h1>
        <p>
            <b>Simple Computer</b><br>
            We explore the organization of a computer which is the structure and function of the components in the computer<br>
            –	Structure: how the components in the computer are connected together, how they communicate<br>
            –	Function: what they do<br>
            Specifically, we will explore the<br>
            –	<b>CPU</b><br>
            –	<b>Memory</b><br>
            –	<b>I/O</b><br>
            –	<b>Bus</b><br>
            –	A primitive instruction set (<b>MARIE</b>)<br>
            –	MARIE is the book’s simple computer<br>
            –	We will examine it to understand what an instruction set is, before we begin our examination of the instruction set for Intel<br><br>
            
            <b>The CPU</b><br>
            The central processing unit (or processor)<br>
            –	Is the brain of the computer<br>
            –	It does all the processing<br>
            The CPU is in charge of executing the current program<br>
            •	Each program is stored in memory along with data<br>
            •	The CPU is in charge of retrieving the next instruction from memory (fetch), decoding and executing it<br>
            •	Execution usually requires the use of ALU and temporary storage in registers<br>
            •	Some instructions cause data movement (memory accesses, input, output) and some instructions change what the next instruction is (branches)<br>
            We divide the CPU into two areas<br>
            •	<b>Datapath</b> – registers and ALU (the execution unit)<br>
            •	<b>Control unit</b> – circuits in charge of performing the fetch-execute cycle.<br><br>
            
            <b>Two Kinds of Registers</b><br>
            - <b>User Register</b><br>
            - <b>Control Register</b><br><br>
            
            <b>ALU</b><br>
            Consists of circuits to perform arithmetic and logic operations<br>
            •	Adder<br>
            •	Multiplier<br>
            •	Shifter<br>
            •	Comparator<br>
            •	Operations in the ALU set status flags (carry, overflow, positive, zero, negative)<br>
            Also, possibly, temporary registers before moving results back to register or memory.<br><br>
            
            <b>Control Unit</b><br>
            Control unit in charge of managing the fetch-execute cycle<br>
            –	It sends out control signals to all other devices.<br>
            –	A control signal indicates that the device should activate or perform its function<br>
            –	For instance:<br>
            Instruction fetching requires<br>
            –	Sending the PC value to main memory<br>
            –	Signaling memory to read<br>
            –	When the datum comes back from memory, move it to the IR<br>
            –	Increment the PC to point to the next instruction<br>
            •	These operations are controlled by the control unit<br>
            •	Now the control unit decodes the instruction signals the proper ALU circuit(s) to execute it<br><br>
            
            <b>The System Clock</b><br>
            In order to regulate when the CU issues its control signals, computers use a system clock. At each clock pulse, the control unit goes on to the next task<br>
            •	Register values are loaded or stored at the beginning of a clock pulse<br>
            •	ALU circuits activate at the beginning of a clock pulse<br>
            Comparing Clocks<br>
            It is difficult to compute CPU performance just by comparing clock speed.<br>
            –	You must also consider how many clock cycles it takes to execute 1 instruction<br>
            •	How fast memory is<br>
            •	How fast the bus is<br>
            In addition, there are different clocks in the computer, the Control Unit and the whole CPU are governed by the system clock<br>
            •	There is usually a bus clock as well to regulate the usage of the slower buses<br><br>
            
            <b>The Bus</b><br>
            Ø	A bus is a collection of wires that allow current to flow over them<br>
            Ø	The current is the information being passed between components<br>
            Ø	There are 3 parts to a bus<br>
            –	<b>data bus</b><br>
            for data and program instructions<br>
            –	<b>control bus</b><br>
            control signals from the CU to the devices, and feedback lines for ack that they are ready or for interrupting the CPU<br>
            –	<b>address bus</b><br>
            the address of the memory location or I/O device that is to perform the given operation<br>
            The Bus<br>
            Additionally, computers may have multiple buses<br>
            Local bus<br>
            connects registers, ALU and CU together<br>
            System bus<br>
            connects CPU to main memory<br>
            Expansion or I/O bus<br>
            connects System bus to I/O devices<br>
            Buses connect two types of devices<br>
            Masters<br>
            Devices that can initiate requests<br>
            •	CPU<br>
            •	some I/O devices<br>
            Slaves<br>
            Devices that only respond to requests from masters<br>
            •	Memory<br>
            •	some I/O devices<br>
            More on Buses<br>
            Some buses are dedicated<br>
            •	The bus directly connects two devices (point-to-point bus)<br>
            Most buses connect multiple components<br>
            •	Multipoint<br><br>
            
            <b>The System Bus</b><br>
            Main memory connects to this bus through pins<br>
            The I/O subsystem connects to this bus through the expansion bus<br>
            The bus carries three types of information<br>
            •	The address from the CPU of the intended item to be accessed<br>
            •	The control information (read versus write, or status information like “are you available?”)<br>
            •	The data, either being sent to the device, or from the device to CPU<br><br>
            
            <b>Expansion Bus</b><br>
            The expansion bus is the collection of expansion slots and what gets plugged into them. Here we see interface cards (or expansion cards), each with the logic to interface between the CPU and the I/O device (e.g., printer, MODEM, disk drive)<br>
            Who gets to use the bus?<br>
            In a point-to-point buses this is not a problem<br>
            In the expansion bus where multiple I/O devices may want to communicate between themselves and the CPU or memory at the same time – we need a form of Bus Arbitration<br>
            –	<b>Daisy chain arbitration</b><br>
            •	Each device has a bus request line on the control bus<br>
            •	When a device wants to use the bus, it places its request and the highest priority device is selected (this is an unfair approach)<br>
            –	<b>Centralized parallel arbitration</b><br>
            •	The bus itself contains an arbiter (a processor) that decides<br>
            •	The arbiter might become a bottleneck, and this is also slightly more expensive<br>
            –	<b>Distributed arbitration</b><br>
            •	Devices themselves determine who gets to use the bus, usually based on a priority scheme, possibly unfair<br>
            –	<b>Distributed arbitration using collision detection</b><br>
            •	It’s a free-for-all, but if a device detects that another device is using the bus, this device waits a short amount of time before trying again<br><br>
            
            <b>I/O Subsystem</b><br>
            •	There are many different types of I/O devices, collectively known as the I/O Subsystem<br>
            •	Since I/O devices can vary in their speed and usage, the CPU does not directly control these devices<br>
            •	Instead, I/O modules, or interfaces, take the CPU commands and pass them on to their I/O devices<br>
            How to communicate with the right I/O device?<br>
            •	To communicate to the right I/O device, the CPU addresses the device through one of two forms<br>
            1.	<b>Memory-mapped I/O</b><br>
            2.	<b>Isolated I/O</b><br><br>
            
            <b>Memory-mapped I/O</b><br>
            q	the interface has its own memory which are addressed as if they were part of main memory, so that some memory locations are not used, they are instead registers in the I/O interfaces<br>
            q	So each of these is given its own address<br>
            q	these addresses overlap those of memory so that a request issued to one of these memory addresses is actually a request of I/O, not memory and memory ignores the request.  In such a system, the addresses are the earliest (say the first 5000 addresses).<br><br>
            
            <b>Isolated I/O</b><br>
            –	In isolated I/O, the 5000 or so addresses are separate from memory, so that we need an extra control line to indicate if the address is a memory address or an I/O address.  In memory-mapped I/O, the early addresses are shared so that, if one of these addresses is sent out, memory ignores it<br><br>
            
            <b>Interrupts</b><br>
            •	CPU performs the fetch-execute cycle on your program repeatedly without pause, until the program terminates<br>
            •	What happens if an I/O device needs attention?<br>
            •	What happens if your program tries to do an illegal operation?<br>
            •	What happens if you want to run 2 or more programs in a multitasking mode?<br>
            –	You cannot do this without interrupts<br>
            •	An interrupt<br>
            –	Is the interruption of the CPU so that it can switch its attention from your program to something else (an I/O device, the operating system)<br><br>
            
            <b>The Interrupt Process</b><br>
            At the end of each fetch-execute cycle, the CPU checks to see if an interrupt has arisen<br>
            –	Devices send interrupts to the CPU over the control bus<br>
            •	If the instruction causes an interrupt, the Interrupt Flag (in the status flags) is set<br>
            –	If an interrupt has arisen, the interrupt is handled as follows<br>
            •	The CPU saves what it was doing (PC and other important registers are saved to the run-time stack in memory)<br>
            •	The CPU figures out who raised the interrupt and executes an interrupt handler to handle that type of interrupt<br>
            –	Interrupt handler is a set of code, stored in memory<br>
            •	Once the interrupt has been handled, the CPU restores the interrupted program by retrieving the values from the run-time stack<br><br>
            
            <b>A Simple Computer</b><br>
            •	We now put all of these elements together into a reduced computer (<b>MARIE</b>)<br>
            •	Machine Architecture that is Really Intuitive & Easy<br>
            •	MARIE is too easy, it is not very realistic, so we will go beyond MARIE as well<br>
            •	We will explore MARIE’s<br>
            –	<b>CPU</b> (registers, ALU, structure)<br>
            –	<b>Instruction set</b> (the instructions, their format – how you specify the instruction, addressing modes used, data types available)<br>
            –	<b>Interrupts, I/O</b><br>
            –	Some simple programs in MARIE<br><br>
            
            <b>MARIE’s Architecture</b><br>
            •	Data stored in binary, two’s complement<br>
            •	Stored programs è stores program data and instructions in same memory<br>
            •	16-bit word size with word addressing (you can only get words from memory, not bytes)<br>
            •	4K of main memory using 12 bit addresses, 16-bit data<br><br>
            
            <b>MARIE’s Architecture</b><br>
            16-bit instructions (4 bits for the op code, 12 bits for the address of the datum in memory)<br><br>
            
            <b>MARIE CPU</b><br>
            The structure of our CPU with the registers shown<br>
            MAR sends to memory, the MBR stores the data being sent to memory or retrieved from memory<br>
            InREG and OutREG receive data from and send data to I/O respectively<br><br>
            
            <b>MARIE’s ISA</b><br>
            A computer’s instruction set architecture specifies the format of its instructions and the primitive operations that the machine can perform.<br>
            The ISA is an interface between a computer’s hardware and its software.<br>
            Some ISAs include hundreds of different instructions for processing data and controlling program execution.<br>
            The MARIE ISA consists of only 13 instructions<br><br>
            
            <b>MARIE’s Instructions</b><br><br>
            
            <b>Assemblers and Assembly Language</b><br>
            Compare the machine code to the assembly code<br>
            You will find the assembly code much easier to decipher<br>
            •	Mnemonics instead of op codes<br>
            •	Variable names instead of memory locations<br>
            •	Labels (for branches) instead of memory locations<br>
            Assembly is an intermediate language between the instruction set (machine language) and the high-level language<br>
            The assembler is a program that takes an assembly language program and assembles it into machine language, much like the compiler compiles a high-level language program<br><br>
            
            <b>Discussion on Assemblers</b><br>
            Mnemonic instructions, such as LOAD 104, are easy for humans to write and understand.<br>
            They are impossible for computers to understand.<br>
            Assemblers translate instructions that are comprehensible to humans into the machine language that is comprehensible to computers<br>
            –	In assembly language, there is a one-to-one correspondence between a mnemonic instruction and its machine code<br>
            –	With compilers, this is not usually the case.<br>
            •	A= B+C<br>
            •	A=add(B,C)<br>
            •	A=B + C<br><br>
            
            <b>Discussion on Assemblers</b><br>
            Assemblers create an object program file from mnemonic source code in two passes<br>
            •	First pass<br>
            –	The assembler assembles as much of the program is it can, while it builds a symbol table that contains memory references for all symbols in the program.<br>
            •	Second pass<br>
            –	The instructions are completed using the values from the symbol table<br>
        </p>
    </div>
    <div id = "P4ITEL202">
        <h1 style="text-align: center;"> <b>Part 4</b></h1>
        <p>
            <b>Logic Gates</b><br>
            <b>AND Gate:</b><br>
            Explanation: The AND gate produces an output of 1 only when both of its inputs are 1; otherwise, it produces an output of 0.<br>
            Truth Table:<br>
            X | Y | Output<br>
            --|---|-------<br>
            0 | 0 |   0<br>
            0 | 1 |   0<br>
            1 | 0 |   0<br>
            1 | 1 |   1<br><br>

            <b>OR Gate:</b><br>
            Explanation: The OR gate produces an output of 1 if at least one of its inputs is 1; otherwise, it produces an output of 0.<br>
            Truth Table:<br>
            X | Y | Output<br>
            --|---|-------<br>
            0 | 0 |   0<br>
            0 | 1 |   1<br>
            1 | 0 |   1<br>
            1 | 1 |   1<br><br>

            <b>NOT Gate (Inverter):</b><br>
            Explanation: The NOT gate (inverter) produces an output that is the opposite of its input. If the input is 1, the output is 0, and if the input is 0, the output is 1.<br>
            Truth Table:<br>
            X | Output<br>
            --|-------<br>
            0 |   1<br>
            1 |   0<br><br>

            <b>NAND Gate:</b><br>
            Explanation: The NAND gate is the opposite of the AND gate. It produces an output of 0 only when both inputs are 1; otherwise, it produces an output of 1.<br>
            Truth Table:<br>
            X | Y | Output<br>
            --|---|-------<br>
            0 | 0 |   1<br>
            0 | 1 |   1<br>
            1 | 0 |   1<br>
            1 | 1 |   0<br><br>

            <b>NOR Gate:</b><br>
            Explanation: The NOR gate is the opposite of the OR gate. It produces an output of 1 only when both inputs are 0; otherwise, it produces an output of 0.<br>
            Truth Table:<br>
            X | Y | Output<br>
            --|---|-------<br>
            0 | 0 |   1<br>
            0 | 1 |   0<br>
            1 | 0 |   0<br>
            1 | 1 |   0<br><br>

            <b>XOR Gate (Exclusive OR):</b><br>
            Explanation: The XOR gate produces an output of 1 if exactly one of its inputs is 1; otherwise, it produces an output of 0.<br>
            Truth Table:<br>
            X | Y | Output<br>
            --|---|-------<br>
            0 | 0 |   0<br>
            0 | 1 |   1<br>
            1 | 0 |   1<br>
            1 | 1 |   0<br><br>

            <b>XNOR Gate (Exclusive NOR):</b><br>
            Explanation: The XNOR gate is the opposite of the XOR gate. It produces an output of 1 if both inputs are the same (either both 0 or both 1); otherwise, it produces an output of 0.<br>
            Truth Table:<br>
            X | Y | Output<br>
            --|---|-------<br>
            0 | 0 |   1<br>
            0 | 1 |   0<br>
            1 | 0 |   0<br>
            1 | 1 |   1<br><br>

            <b>De Morgan's Theorem for AND Gate:</b><br>
            Explanation: De Morgan's Theorem states that the complement of the product of two variables is equal to the sum of the complements of the variables. In the context of logic gates, it means that the complement of the AND operation between two variables is equivalent to the OR operation of the complements of those variables.<br>
            Truth Table:<br>
            <table border="1">
              <tr>
                <th>X</th>
                <th>Y</th>
                <th>X AND Y</th>
                <th>NOT X</th>
                <th>NOT Y</th>
                <th>NOT X OR NOT Y</th>
                <th>De Morgan's Theorem</th>
              </tr>
              <tr>
                <td>0</td>
                <td>0</td>
                <td>0</td>
                <td>1</td>
                <td>1</td>
                <td>1</td>
                <td>1</td>
              </tr>
              <tr>
                <td>0</td>
                <td>1</td>
                <td>0</td>
                <td>1</td>
                <td>0</td>
                <td>1</td>
                <td>1</td>
              </tr>
              <tr>
                <td>1</td>
                <td>0</td>
                <td>0</td>
                <td>0</td>
                <td>1</td>
                <td>1</td>
                <td>1</td>
              </tr>
              <tr>
                <td>1</td>
                <td>1</td>
                <td>1</td>
                <td>0</td>
                <td>0</td>
                <td>0</td>
                <td>0</td>
              </tr>
            </table><br>
            
            <b>De Morgan's Theorem for OR Gate:</b><br>
            Explanation: De Morgan's Theorem states that the complement of the sum of two variables is equal to the product of the complements of the variables. In the context of logic gates, it means that the complement of the OR operation between two variables is equivalent to the AND operation of the complements of those variables.<br>
            Truth Table:<br>
            <table border="1">
              <tr>
                <th>X</th>
                <th>Y</th>
                <th>X OR Y</th>
                <th>NOT X</th>
                <th>NOT Y</th>
                <th>NOT X AND NOT Y</th>
                <th>De Morgan's Theorem</th>
              </tr>
              <tr>
                <td>0</td>
                <td>0</td>
                <td>0</td>
                <td>1</td>
                <td>1</td>
                <td>1</td>
                <td>1</td>
              </tr>
              <tr>
                <td>0</td>
                <td>1</td>
                <td>1</td>
                <td>1</td>
                <td>0</td>
                <td>0</td>
                <td>0</td>
              </tr>
              <tr>
                <td>1</td>
                <td>0</td>
                <td>1</td>
                <td>0</td>
                <td>1</td>
                <td>0</td>
                <td>0</td>
              </tr>
              <tr>
                <td>1</td>
                <td>1</td>
                <td>1</td>
                <td>0</td>
                <td>0</td>
                <td>0</td>
                <td>0</td>
              </tr>
            </table><br>
        </p>
    </div>
    <div id = "P5ITEL202">
        <h1 style="text-align: center;"> <b>Part 5</b></h1>
        <b>A Closer Look at Instruction Set Architectures</b><br><br>

        <b>Introduction</b><br>
        We present a detailed look at different instruction formats, operand types, and memory access methods. We will see the interrelation between machine organization and instruction formats. This leads to a deeper understanding of computer architecture in general.<br><br>

        <b>Instruction Formats</b><br><br>

        Instruction sets are differentiated by the following:<br>
        - Number of bits per instruction.<br>
        - Stack-based or register-based.<br>
        - Number of explicit operands per instruction.<br>
        - Operand location.<br>
        - Types of operations.<br>
        - Type and size of operands.<br><br>

        Instruction set architectures are measured according to:<br>
        - Main memory space occupied by a program.<br>
        - Instruction complexity.<br>
        - Instruction length (in bits).<br>
        - Total number of instructions in the instruction set.<br><br>

        In designing an instruction set, consideration is given to:<br>
        - Instruction length.<br>
        - Whether short, long, or variable.<br>
        - Number of operands.<br>
        - Number of addressable registers.<br>
        - Memory organization.<br>
        - Whether byte- or word-addressable.<br>
        - Addressing modes.<br>
        - Choose any or all: direct, indirect or indexed.<br><br>

        As an example, suppose we have the hexadecimal number 12345678.<br>
        The big-endian and little-endian arrangements of the bytes are shown below:<br><br>

        Address 00, 01, 10, 11<br>
        Big Endian: 12, 34, 56, 78<br>
        Little Endian: 78, 56, 34, 12<br><br>

        Big endian:<br>
        - Is more natural.<br>
        - The sign of the number can be determined by looking at the byte at address offset 0.<br>
        - Strings and integers are stored in the same order.<br><br>

        Little endian:<br>
        - Makes it easier to place values on non-word boundaries.<br>
        - Conversion from a 16-bit integer address to a 32-bit integer address does not require any arithmetic.<br><br>

        The next consideration for architecture design concerns how the CPU will store data.<br>
        We have three choices:<br>
        1. A stack architecture<br>
        2. An accumulator architecture<br>
        3. A general-purpose register architecture.<br>
        In choosing one over the other, the tradeoffs are simplicity (and cost) of hardware design with execution speed and ease of use.<br><br>

        In a stack architecture, instructions and operands are implicitly taken from the stack.<br>
        - A stack cannot be accessed randomly.<br><br>

        In an accumulator architecture, one operand of a binary operation is implicitly in the accumulator.<br>
        - One operand is in memory, creating lots of bus traffic.<br><br>

        In a general-purpose register (GPR) architecture, registers can be used instead of memory.<br>
        - Faster than accumulator architecture.<br>
        - Efficient implementation for compilers.<br>
        - Results in longer instructions.<br><br>

        Most systems today are GPR systems.<br>
        There are three types:<br>
        - Memory-memory where two or three operands may be in memory.<br>
        - Register-memory where at least one operand must be in a register.<br>
        - Load-store where no operands may be in memory.<br><br>

        The number of operands and the number of available registers have a direct effect on instruction length.<br><br>

        Stack machines use one- and zero-operand instructions.<br>
        LOAD and STORE instructions require a single memory address operand.<br>
        Other instructions use operands from the stack implicitly.<br>
        PUSH and POP operations involve only the stack’s top element.<br>
        Binary instructions (e.g., ADD, MULT) use the top two items on the stack.<br><br>

        Stack architectures require us to think about arithmetic expressions a little differently.<br>
        We are accustomed to writing expressions using infix notation, such as: Z = X + Y.<br>
        Stack arithmetic requires that we use postfix notation: Z = XY+.<br>
        - This is also called reverse Polish notation, (somewhat) in honor of its Polish inventor, Jan Lukasiewicz (1878 - 1956).<br><br>

        The principal advantage of postfix notation is that parentheses are not used.<br>
        For example, the infix expression,<br>
        Z = (X x Y) + (W x U),<br>
        becomes:<br>
        Z = X Y x W U x +<br>
        in postfix notation.<br><br>

        In a stack ISA, the postfix expression,<br>
        Z = X Y x W U x +,<br>
        might look like this:<br>
        PUSH X<br>
        PUSH Y<br>
        MULT<br>
        PUSH W<br>
        PUSH U<br>
        MULT<br>
        ADD<br>
        PUSH Z<br><br>

        In a one-address ISA, like MARIE, the infix expression,<br>
        Z = X x Y + W x U,<br>
        looks like this:<br>
        LOAD X<br>
        MULT Y<br>
        STORE TEMP<br>
        LOAD W<br>
        MULT U<br>
        ADD TEMP<br>
        STORE Z<br><br>

        In a two-address ISA (e.g., Intel, Motorola), the infix expression,<br>
        Z = X x Y + W x U,<br>
        might look like this:<br>
        LOAD R1, X<br>
        MULT R1, Y<br>
        LOAD R2, W<br>
        MULT R2, U<br>
        ADD R1, R2<br>
        STORE Z, R1<br><br>

        With a three-address ISA (e.g., mainframes), the infix expression,<br>
        Z = X x Y + W x U,<br>
        might look like this:<br>
        MULT R1, X, Y<br>
        MULT R2, W, U<br>
        ADD Z, R1, R2<br><br>

        We have seen how instruction length is affected by the number of operands supported by the ISA.<br>
        In any instruction set, not all instructions require the same number of operands.<br>
        Operations that require no operands, such as HALT, necessarily waste some space when fixed-length instructions are used.<br>
        One way to recover some of this space is to use expanding opcodes.<br>
      
        <b>A system has 16 registers and 4K of memory.</b><br>
        We need 4 bits to access one of the registers. We also need 12 bits for a memory address.<br>
        If the system is to have 16-bit instructions, we have two choices for our instructions:<br><br>

        <b>Instruction Types</b><br>
        Instructions fall into several broad categories that you should be familiar with:<br>
        - Data movement.<br>
        - Arithmetic.<br>
        - Boolean.<br>
        - Bit manipulation.<br>
        - I/O.<br>
        - Control transfer.<br>
        - Special purpose.<br><br>

        <b>ADDRESSING</b><br><br>

        Addressing modes specify where an operand is located.<br>
        They can specify a constant, a register, or a memory location.<br>
        The actual location of an operand is its effective address.<br>
        Certain addressing modes allow us to determine the address of an operand dynamically.<br><br>

        Immediate addressing is where the data is part of the instruction.<br>
        Direct addressing is where the address of the data is given in the instruction.<br>
        Register addressing is where the data is located in a register.<br>
        Indirect addressing gives the address of the address of the data in the instruction.<br>
        Register indirect addressing uses a register to store the address of the address of the data.<br><br>

        Immediate addressing is where the data is part of the instruction.<br>
        Direct addressing is where the address of the data is given in the instruction.<br>
        Register addressing is where the data is located in a register.<br>
        Indirect addressing gives the address of the address of the data in the instruction.<br>
        Register indirect addressing uses a register to store the address of the address of the data.<br><br>

        Indexed addressing uses a register (implicitly or explicitly) as an offset, which is added to the address in the operand to determine the effective address of the data.<br>
        Based addressing is similar except that a base register is used instead of an index register.<br>
        The difference between these two is that an index register holds an offset relative to the address given in the instruction, a base register holds a base address where the address field represents a displacement from this base.<br><br>

        In stack addressing the operand is assumed to be on top of the stack.<br>
        There are many variations to these addressing modes including:<br>
        - Indirect indexed.<br>
        - Base/offset.<br>
        - Self-relative<br>
        - Auto increment - decrement.<br>
        We won’t cover these in detail.<br><br>

        <b>Instruction-Level Pipelining</b><br><br>

        Some CPUs divide the fetch-decode-execute cycle into smaller steps.<br>
        These smaller steps can often be executed in parallel to increase throughput.<br>
        Such parallel execution is called instruction-level pipelining.<br>
        This term is sometimes abbreviated ILP in the literature.<br><br>

        Suppose a fetch-decode-execute cycle were broken into the following smaller steps:<br>
        1. Fetch instruction<br>
        2. Decode opcode<br>
        3. Calculate effective address of operands.<br>
        4. Fetch operands<br>
        5. Execute instructions<br>
        6. Store result<br><br>

        Suppose we have a six-stage pipeline.  S1 fetches the instruction, S2 decodes it, S3 determines the address of the operands, S4 fetches them, S5 executes the instruction, and S6 stores the result<br><br>

        For every clock cycle, one small step is carried out, and the stages are overlapped.<br>
        The theoretical speedup offered by a pipeline can be determined as follows:<br>
        Let tp be the time per stage.  Each instruction represents a task, T, in the pipeline.<br>
        The first task (instruction) requires k x tp time to complete in a k-stage pipeline.  The remaining (n - 1) tasks emerge from the pipeline one per cycle.  So the total time to complete the remaining tasks is (n - 1)tp.<br>
        Thus, to complete n tasks using a k-stage pipeline requires:<br>
        (k x tp) + (n - 1)tp = (k + n - 1)tp.<br><br>

        If we take the time required to complete n tasks without a pipeline and divide it by the time it takes to complete n tasks using a pipeline, we find:<br>
        Speedup S = ntn/(k + n - 1) tp<br><br>

        If we take the limit as n approaches infinity, (k + n - 1) approaches n, which results in a theoretical speedup of:<br>
        Speedup S = ktp/tp = k<br><br>

        Our neat equations take a number of things for granted.<br>
        First, we have to assume that the architecture supports fetching instructions and data in parallel.<br>
        Second, we assume that the pipeline can be kept filled at all times.  This is not always the case.  Pipeline hazards arise that cause pipeline conflicts and stalls.<br><br>

        An instruction pipeline may stall, or be flushed for any of the following reasons:<br>
        - Resource conflicts.<br>
        - Data dependencies.<br>
        - Conditional branching.<br>
        Measures can be taken at the software level as well as at the hardware level to reduce the effects of these hazards, but they cannot be totally eliminated.<br><br>

        <b>Real-World Example of ISAs</b><br><br>

        -We return briefly to the Intel and MIPS architectures from the last chapter, using some of the ideas introduced in this chapter.<br>
        -Intel introduced pipelining to their processor line with its Pentium chip.<br>
        -The first Pentium had two five-stage pipelines.  Each subsequent Pentium processor had a longer pipeline than its predecessor with the Pentium IV having a 24-stage pipeline.<br>
        -The Itanium (IA-64) has only a 10-stage pipeline.<br>
        -Intel processors support a wide array of addressing modes.<br>
        -The original 8086 provided 17 ways to address memory, most of them variants on the methods presented in this chapter.<br>
        -Owing to their need for backward compatibility, the Pentium chips also support these 17 addressing modes.<br>
        -The Itanium, having a RISC core, supports only one: register indirect addressing with optional post increment.<br>
        -Intel processors support a wide array of addressing modes.<br>
        -The original 8086 provided 17 ways to address memory, most of them variants on the methods presented in this chapter.<br>
        -Owing to their need for backward compatibility, the Pentium chips also support these 17 addressing modes.<br>
        -The Itanium, having a RISC core, supports only one: register indirect addressing with optional post increment.<br>
        -MIPS was an acronym for Microprocessor Without Interlocked Pipeline Stages.<br>
        -The architecture is little endian and word-addressable with three-address, fixed-length instructions.<br>
        -Like Intel, the pipeline size of the MIPS processors has grown: The R2000 and R3000 have five-stage pipelines.; the R4000 and R4400 have 8-stage pipelines.<br>
        -The R10000 has three pipelines: A five-stage pipeline for integer instructions, a seven-stage pipeline for floating-point instructions, and a six-state pipeline for LOAD/STORE instructions. <br>
        -In all MIPS ISAs, only the LOAD and STORE instructions can access memory.<br>
        -The ISA uses only base addressing mode.<br>
        -The assembler accommodates programmers who need to use immediate, register, direct, indirect register, base, or indexed addressing modes.<br>
        -The Java programming language is an interpreted language that runs in a software machine called the Java Virtual Machine (JVM).<br>
        -A JVM is written in a native language for a wide array of processors, including MIPS and Intel.<br>
        -Like a real machine, the JVM has an ISA all of its own, called bytecode. This ISA was designed to be compatible with the architecture of any machine on which the JVM is running.<br>
        -Java bytecode is a stack-based language.<br>
        -Most instructions are zero address instructions.<br>
        -The JVM has four registers that provide access to five regions of main memory.<br>
        -All references to memory are offsets from these registers. Java uses no pointers or absolute memory references.<br>
        -Java was designed for platform interoperability, not performance!<br><br>
        <b>Conclusion</b><br>
        ISAs are distinguished according to their bits per instruction, number of operands per instruction, operand location and types and sizes of operands.<br>
        Endianness as another major architectural consideration.<br>
        CPU can store store data based on<br>
        1. A stack architecture<br>
        2. An accumulator architecture<br>
        3. A general purpose register architecture.<br><br>
        Instructions can be fixed length or variable length.<br>
        To enrich the instruction set for a fixed length instruction set, expanding opcodes can be used.<br>
        The addressing mode of an ISA is also another important factor.  We looked at:<br>
        - Immediate	– Direct<br>
        - Register		– Register Indirect<br>
        - Indirect		– Indexed<br>
        - Based		– Stack<br><br>

        A k-stage pipeline can theoretically produce execution speedup of k as compared to a non-pipelined machine.<br>
        Pipeline hazards such as resource conflicts and conditional branching prevents this speedup from being achieved in practice.<br>
        The Intel, MIPS, and JVM architectures provide good examples of the concepts presented in this chapter.
    </div>
    <div id = "P6ITEL202">
        <h1 style="text-align: center;"> <b>Part 6</b></h1>
        <p>
            <b>Memory Organization</b><br><br>

            <b>Outline</b><br><br>
            • Hierarchical Memory Systems<br>
            • Cache Memory<br>
            • Virtual Memory<br>
            • Pentium/Windows Memory System<br><br>

            <b>Memory Hierarchy</b><br>
            CPU with L1 cache <-> L2 cache <-> Physical Memory <-> Virtual Memory Storage<br><br>


            <b>Associative Memory</b><br><br>
            • <b>Data Register</b><br>
            A data register is a small, high-speed storage location within a CPU (Central Processing Unit) that temporarily holds data that the CPU is currently processing. These registers are part of the CPU's architecture and are built directly into the processor.<br>
            Registers are the fastest type of memory in a computer system and are used for various purposes.<br><br>

            <b>Data Register</b><br>
            A data register is a storage component within the CPU (Central Processing Unit) that temporarily holds data that is being processed or manipulated by the CPU. These registers are used for various purposes, such as storing operands for arithmetic or logical operations, holding intermediate results of calculations, or buffering data during input/output operations. They are fundamental to the CPU's operations and are optimized for high-speed data manipulation.<br><br>

            <b>Mask Register</b><br>
            A mask register is a register used in computing to perform bitwise logical operations, such as AND, OR, or XOR, on data stored in other registers. The mask register contains a bit pattern that is applied during these operations to selectively modify or filter specific bits from the data in another register. This enables the manipulation of specific bits within a register without affecting the entire content, often used in tasks that require selective modification or extraction of certain bits.<br><br>

            <b>Match Register</b><br>
            A match register is a component used in various systems, particularly in networking or pattern matching applications, to compare and detect matches between patterns or data sets. It typically stores a specific pattern or a key against which other data or patterns are compared. When data is compared against the content of the match register, if there is a match, it signals or triggers a response within the system. Match registers are used in various applications, including network security for intrusion detection systems or in data processing for pattern recognition.<br><br>

            <b>Data Lines/Blocks</b><br><br>
            • Multiple consecutive words form a line<br>
            • Data lines or data blocks typically refer to a sequence of bytes or units of data in a computer system. In computer architecture and storage systems, data is often organized into blocks or lines for efficient processing and retrieval.<br>
            • A data line or block is a fundamental unit of data transfer between different components within a computer system. It represents a contiguous sequence of bytes or bits that are read from or written to memory, storage devices, or transferred between different parts of a processor.<br><br>

            <b>Example 1: MM Size = 4 GB</b><br>
            Cache Size = 1 MB<br>
            Block Size = 4KB<br><br>

            1. P.A. bits split?<br>
            2. Tag directory size?<br><br>

            Sol. MM Size = 4 GB = 22 x 230 B = 2(2+30) B= 232 B No. of P.A. bits = log, 232 = 32<br>
            Block Size = 4 KB = 22 x 210 B = 212 B<br>
            No. of Blocks in MM = 232/212 = 220<br>
            No. of Tag bits = log2 220 = 20<br>
            Cache Size = 1 MB = 1 x 220 B = 220 B<br>
            No. of Lines in Cache = 220/212 = 28<br><br>

            Cache Size = ?<br>
            No. of Lines in Cache = ?<br><br>

            Block offset = 14<br>
            34 bits<br>
            20 bits  I 14 bits<br><br>

            In a direct-mapped cache, when the CPU needs to access data from memory<br>
            • The memory address is divided into the tag, index, and offset.<br>
            • The index part is used to locate a specific line in the cache.<br>
            • The tag from the address is compared to the stored tag in that line to check if the requested data is present in the cache. If the tags match, it's a cache hit. If not, it's a cache miss.<br>
            • If it's a cache miss, the required block of data from the main memory is loaded into the cache line determined by the index, overwriting any existing data in that slot.<br><br>

            <b>Advantages:</b> and <b>Disadvantages:</b><br><br>

            <b>Advantages:</b><br>
            • Simplicity: Direct-mapped caches are relatively simple to implement compared to other mapping strategies.<br>
            • Deterministic: Given a memory address, the location in the cache where the corresponding data will be stored or retrieved is predictable.<br><br>

            <b>Disadvantages:</b><br>
            • Increased chance of conflicts: As each block of main memory can only be stored in a specific location in the cache, if multiple memory blocks map to the same cache line (due to hashing or modulo operations), it leads to conflicts and cache thrashing.<br>
            • Lower hit rate: Depending on the memory access patterns, a direct-mapped cache may have a higher likelihood of cache misses compared to other cache designs like set-associative or fully associative caches.<br><br>

            <b>Drawbacks of Direct Mapped Cache</b><br>
            0000 0000 0000 0000: JUMP 1000H<br>
            0001 0000 0000 0000: JUMP 0000H<br>
            Direct-mapped caches offer simplicity and speed but come with some drawbacks:<br><br>

            <b>How FIFO Works:</b><br><br>
            1. Tracking Entry Time: Each cache line has associated metadata, typically a timestamp or a counter, indicating when the block was brought into the cache or accessed last.<br>
            2. Replacement Decision: When a new block needs to be inserted into a full cache, FIFO selects the block that has been in the cache the longest, based on the timestamp or the order in which it entered the cache.<br>
            3. Eviction: The block selected by the FIFO policy is then replaced with the new block. This is done without considering the usefulness of the data or its frequency of access. It strictly adheres to the principle that the first block to enter the cache is the first to be evicted.<br><br>

            <b>Advantages of FIFO:</b><br>
            • Simplicity: FIFO is easy to implement and manage. It doesn't require complex tracking mechanisms or algorithms to determine which block should be replaced.<br>
            • Predictability: As it follows a straightforward rule of replacing the oldest block, its behavior is predictable and consistent under similar conditions.<br><br>

            <b>Drawbacks of FIFO:</b><br>
            • Belated Replacement: FIFO may evict blocks that are frequently accessed, simply because they were the first to be brought into the cache. This can lead to poor cache utilization and increased cache misses if the replacement policy does not consider the frequency of access or the relevance of the data.<br>
            • Sequential Access Issues: In situations where data is accessed in a sequential manner, FIFO may cause cache pollution by retaining outdated or less-relevant data and replacing it with newer but similarly sequential data.<br>
            • Performance Impact: FIFO might not always be the most efficient policy, especially in scenarios where the access patterns do not align well with the order of block insertion into the cache.<br><br>

            FIFO is a straightforward and easy-to-implement replacement policy, but its limitations, especially in handling varying access patterns and relevance of data, have led to the development of more sophisticated replacement algorithms like LRU (Least Recently Used), LFU (Least Frequently Used), and others, which aim to make more informed decisions about which blocks to retain or evict from the cache.<br>
        </p>
    </div>
    <script src="Navigation.js"></script>
</div>
</body>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ITEP203</title>
    <link rel="website icon" type="jpg" href="icon.jpg">
    <link rel="stylesheet" href="ITEP203.css">
</head>
<body>
  <!--Header-->
  <div id="header">
    <div id="mySidenav" class="sidenav">
      <a href="javascript:void(0)" class="closebtn" onclick="closeNav()">&times;</a>
      <a href="Homepage.html">Home</a>
        <a href="home.html">Reviewer</a>
        <a href="About.html">About Us</a>
        <a href="Contact.html">Contact</a>
    </div>
    <span onclick="openNav()">&#9776; ITEP203</span>
    <script src="Navigation.js"></script>
</div>
  <!--Contents-->
  <div id = "content">
      <h1 style="text-align: center;">Quantitative Methods Including Modeling and Simulation</h1>
    <div id = "P1ITEP203">
        <h1 style="text-align: center;"> <b>Part 1</b></h1>
        <p><b>ESSENTIALS OF QUANTITATIVE RESEARCH</b><br>
        
            <b>What is Quantitative Research?</b><br>
            • Quantitative research involves the use of empirical methods to investigate a particular social and the phenomenon or research question, the data of which will be amenable to the use of numerical and statistical techniques in the analysis. The data that is collected is either numerical, or can be converted to numerical values. The data is analyzed through the use of relevant statistical techniques.<br>
            • Quantitative researchers define in advance the particular topic they plan to study along with the current status in the existing research literature and the suitable methodology to study the same as well. Thus the study design is pre planned allowing only for such changes that may be required due to unforeseen circumstances.<br>
            • In this sense, the existing theory will be tested and validated providing for explanations about why and how a phenomenon occurs in a particular context.<br>
            For example, with the help of the available theory, the researcher could explain why the sample he/she studied was feeling excluded from their social group they belonged to.<br>
            • This approach of making explanations about the topic studied based on the available theory is known as the <b>deductive approach</b> and this is the hallmark of the quantitative methodology.<br><br>
            
            <b>The following research questions can be studied quantitatively:</b><br>
            ● What percentages of students among those who complete vocational training find gainful employment within a year of completing training?<br>
            ● How many children among those who complete primary schooling enter secondary school?<br>
            ● What is the difference in the levels of parental education, occupation and monthly income of the children attending the primary section of a municipal school and a private school in Mumbai?<br>
            For example, assume that we want to measure the attitudes of adolescent girls towards the practice of dowry, through the survey method. While preparing the attitude scale we can place the attitudinal statements and pre determine the respondents’ answers to them in such a way that they are amenable to the use of numerical values and hence quantification.<br>
            <b>Statement:</b> The dowry system is a social evil in India.<br>
            ● Strongly Agree<br>
            ● Agree<br>
            ● Neither Agree or Disagree<br>
            ● Disagree<br>
            ● Strongly Disagree<br>
            Similarly, we can also easily calculate the actual range of scores by multiplying the lowest and highest scores possible (in this example, 1 and 5) with the total number of statements in the scale. As we have five statements, the range will be between 5(1x5) and 25(5x5).<br>
            We can even decide suitable class intervals based on this range to differentiate the degree of variation in attitudes. Continuing with the previous example, we can make three class intervals within the range of 5 to 25 such as the scores of 5 to 11 representing strong negative view about dowry, 12 to 18 representing moderately negative view about dowry and 19 to 25 representing strong positive view about the issue.<br>
            After the analysis, it may be possible to say that 68 percent of the respondents had strong negative attitude towards the idea of practicing dowry, while 30 percent had moderately negative attitude; just two percent showed strongly favourable attitude towards dowry.<br>
            Quantitative researchers are careful about maintaining the objectivity of their research, and not allowing their own presence, behavior or biases to affect their research process.<br>
            For example, in the results from our previous example of adolescent girls’ attitudes to dowry may only hold true to the girls from a particular school because the study may not have included a sample of girls from diverse geographical locations. Thus, the researchers cannot generalize or attribute these results to all adolescent girls in a particular district or state.<br>
            Quantitative research methods usually enable collecting data from large samples with predictable accuracy and in such instances the research results obtained can be generalized to even larger populations with similar characteristic features. Because it is generalizable, quantitative methods are often used to collect field data that can be used to design policies or interventions for large populations.<br>
            For example, the National Family Health Survey can be used to design health policies and health interventions because a large amount of data is collected from representative populations across the country.<br><br>
            
            <b>Key components of Quantitative Research Methods</b><br>
            ● Units: The people or things we collect research data on are called units, or research units.<br>
            <b>Example of units are:</b><br>
            • human such as, children, tribal, working women, college students, workers in unorganized sector and so on<br>
            • non-human entities like schools, villages, houses, factories, colleges, NGOs, hospitals and so on.<br><br>
            
            <b>Variables:</b> Variables are the specific characteristics of the units that we are interested in researching. As suggested by its name, variables have values that vary. They vary in name, type, degree, number and so on.<br>
            <b>Examples of variables are:</b><br>
            • age<br>
            • gender<br>
            • educational level<br>
            • income<br>
            • type of occupation<br>
            • level of awareness<br>
            • level of participation and so on<br>
            Variables are classified into different types depending on the purpose it serves in a given research study.<br>
            • <b>Independent variables</b> are variables that influence or affect another variable.<br>
            • <b>Dependent variables</b> are variables that are affected by variations in the Independent variable.<br><br>
            
            ● <b>Sample</b>: A sample is a subset of a total number of individuals/ institutions/ villages/ towns/ households/ articles and so on from whom data is collected in a research study. In quantitative studies, data generated from a sample is used to make observations and inferences about the larger population.<br><br>
            
            <b>Hypothesis:</b> A hypothesis is a statement that explains the relationship between two or more variables, the validity of which needs to be tested with the help of empirical data.<br>
            This statement is tested during the research study. In quantitative research, a hypothesis is usually based on previous research findings. An example of a hypothesis is “The higher the educational level of the women, the higher their income level.”<br><br>
            
            <b>Types of Quantitative Research</b><br>
            <b>Descriptive</b><br>
            Descriptive research describes or quantifies identified variables. They typically seek to answer questions that describe certain phenomena. They sometimes involve questions such as “how much?” or “what percentage?” or “how often?” Descriptive research collects data on the status of things and uses this data to analyze the research question. Some examples of descriptive research questions are:<br>
            • What percentage of rag pickers are girls?<br>
            • How often do adolescents use social networks on a monthly basis?<br>
            • How frequently do children employed in home-based factories go to school each month?<br>
            • What is the extent of cigarette smoking among 18-25 year old Indians?<br>
            Descriptive research designs generally attempt to test variable relationships or causality between variables.<br><br>
            
            <b>Correlational</b><br>
            Correlational research attempts to determine to what extent two or more variables are related to each other. This type of study explores patterns and trends in the data, but may not be able to prove any causal links between the variables. Because of this, generally speaking, there is no manipulation of variables in this type of study– they are only studied in their existing states. Some examples of correlational research themes are:<br>
            • What is the relationship between volunteering and self-esteem?<br>
            • What is the relationship between smoking and age of the person?<br>
            • What is the relationship between maternal education levels and family size?<br>
            • What is the relationship between malnutrition and family income levels?<br><br>
            
            <b>Cause-Comparative</b><br>
            Cause-comparative studies aim to establish a causal relationship between two or more variables. They are also known as quasi-experimental research designs. Although this type of study shares some similarities with Experimental research design, it is different because in this type of study, there is no randomized assignment of subjects in sample to control and experimental groups.<br>
            Instead, researchers focus on comparing groups who have been exposed to certain treatments/interventions to other groups that have not had this exposure. Additionally, some quasi-experimental studies do not require the manipulation of the independent variable.<br>
            Researchers undertaking this type of study have to be very careful in attributing causal relationships between variables because there may be external variables (which may or may not be evident to the researchers) which may be influencing the causal relationship. Some examples of cause-comparative research:<br>
            • The influence of preschool education on primary school completion<br>
            • The effect of smoking on lung cancer<br>
            • The effect of education levels on income<br>
            • The effect of poverty on mental health<br>
            • The effect of tutoring on the academic grades of children in Class 5.<br>
            Quasi-experimental designs are particularly useful in those cases where it is not practical or is unethical to conduct an Experimental research.<br>
            However, quasi- experimental research studies are also subject to issues of internal validity because the control and experiment group (or pre intervention group and post intervention group, as they are sometimes known) may not have been exactly comparable or equal in their characteristics, and this may have influenced the study’s results and the causal relationship between the variables.<br><br>
            
            <b>Experimental</b><br>
            Experimental research is often called “true experimentation” or the gold standard of empirical studies. In these types of studies, the independent variable is manipulated to assess causal relationships, and to determine that any variation in the dependent variable is actually caused by the identified variable, and not by some external variables.<br>
            Another unique aspect of this type of study is that subjects are randomly assigned to control or experiment groups. At the start of the experiment, the identified subjects are as identical in their characteristics as possible, and then they are randomly assigned to a group that will receive a treatment or intervention (known as the experimental group) or to a group that does not receive a treatment or intervention (known as the control group).<br>
            Some examples of experimental research are as follows:<br>
            • The effect of classroom teaching and one-on-one teaching on children’s academic grades<br>
            • The effect of “punishment free” education on children’s self-esteem<br>
            • The effect of a new drug on HIV positive patients<br>
            Thus, if a certain drug is found to benefit an experimental group of HIV positive patients, chances are that the drug may prove beneficial to populations who share the overall characteristics of the subjects in the sample. Because experimental research has the advantage of proving causality to an extent that other research designs cannot, social scientists are increasingly using this design to study causal relationships.<br><br>
            
            <b>Tools to Conduct Quantitative Research</b><br>
            • The most common tool used in quantitative studies is surveys. Surveys can be conducted over the phone, online, through mail, in-person, or may be self-administered by participants.<br>
            • Some researchers also use fixed panel studies or cohort studies. These types of studies focus on a certain cohort (or cohorts) over a particular period of time.<br><br>
            <b>Introduction to Modeling and Simulation</b><br>
                Modeling and simulation constitute a powerful method for designing and evaluating complex systems and processes, and knowledge of modeling and simulation principles is essential to many analysts and project managers as they engage in state-of-the-art research and development.<br>
                The process begins with careful problem formulation and model construction and continues with simulation experiments, interpretation of results, validation of the model, documentation of the results, and final implementation of the study's conclusions. Each step must be approached systematically; none can be safely omitted. Critical issues are identified at each step, and guidelines are presented for the successful completion of modeling and simulation studies.<br>
                Constructing abstractions of systems (models) to facilitate experimentation and assessment (simulation) is both art and science. The technique is particularly useful in solving problems of complex systems where easier solutions do not present themselves. Modeling and simulation methods also allow experimentation that otherwise would be cumbersome or impossible.<br>
                As powerful as modeling and simulation methods can be, applying them haphazardly can lead to erroneous conclusions.<br>
                Despite a firm foundation in mathematics, computer science, probability, and statistics, the discipline remains intuitive. For example, the issues most relevant in a cardiology study may be quite different from those most significant in a military effectiveness study.<br><br>

                <b>SYSTEMS, MODELS, AND SOLUTIONS</b><br>
                Modeling and simulation are used to study systems. A system is defined as the collection of entities that make up the facility or process of interest. To facilitate experimentation and assessment of a system, a representation - called a model - is constructed.<br>
                Physical models represent systems as actual hardware, whereas mathematical models represent systems as a set of computational or logical associations.<br>
                Models can also be static, representing a system at a particular point in time, or dynamic, representing how a system changes with time. The set of variables that describes the system at any particular time is called a state vector. In general, the state vector changes in response to system events. In continuous systems state vectors change constantly, but in discrete systems they change only a finite number of times.<br>
                When at least one random variable is present, a model is called stochastic; when random variables are absent, a model is called deterministic.<br>
                Sometimes the mathematical relationships describing a system are simple enough to solve analytically. Analytical solutions provide exact information regarding model performance.<br>
                When the model cannot be solved analytically, it can often imitate system operations a process called simulation-enabling performance to be estimated numerically.<br>
                In general, systems are modeled as having a number of inputs (or stimuli), X1, X2, … , Xr; a number of outputs (or responses), Y1, Y2, ... , Ys; and a number of system parameters (or conditions), P1, P2,···, Pc.<br>
                Output could include a characterization of the delays incurred by messages in queues and total end-to-end delivery time. Thus, a system is often viewed as a function f that produces output y from inputs x and system parameters p; that is, y = f(x, p).<br><br>

                <b>Common Applications</b><br>
                Modeling and simulation studies tend to fall into four application categories: proof-of-concept, modification, comparison, and optimization.<br>
                Proof-of concept studies are done during the predesign phase of a future system. Modeling and simulation help determine the viability of concepts and provide insight into expected system performance.<br>
                Modification studies are done on existing systems to allow inferences regarding system performance under proposed operating conditions and to allow parameter settings to be tuned for desired system prediction accuracy. For example, alterations to existing military weapons systems are often evaluated for their effectiveness against new threats.<br>
                Comparison studies involve competing systems. For example, two different systems for producing monetary gain through the trade of financial securities could be evaluated for their relative performance during simulated economic conditions.<br>
                Finally, optimization studies are used to determine the best system operating conditions. For example, in manufacturing facilities, modeling and simulation can help determine the workstation layout that produces the best trade-off between productivity and cost.<br><br>

                <b>Solving Problems Using Modeling and Simulation</b><br>
                This process has the following major steps:<br>
                1. Formulate problem and model. Write a problem statement, model system attributes, and select a solution technique (presumably modeling and simulation).<br>
                2. Construct system model. Next, construct conceptual and computerized models and model input data.<br>
                3. Conduct simulation experiments. Design experiments and collect data.<br>
                4. Interpret results. Statistically analyze output data and assess their relevance to the study's objectives.<br>
                5. Document study. Write a description of the analysis and by-products associated with performing each major step in the problem-solving process.<br>
                6. Implement conclusions. Act upon the decisions produced by the study.<br>
                7. Validate model. During each step, assess the accuracy of the analysis<br><br>

                Validation is the ongoing attempt to ensure high credibility of study conclusions.<br>
                Problem solving, however, is rarely a sequential process. Validation must be an ongoing process, and, as the figure shows, it applies to all steps. Furthermore, insights gained at any step may require a move "backward" to adjust previous work. The blue lines show the possibility of revisiting previous work.<br>
        </p>
    </div>
    <div id = "P2ITEP203">
        <h1 style="text-align: center;"> <b>Part 2</b></h1>
        <p>
            <b>Descriptive Statistics</b><br>
            <b>WHAT IT IS</b><br>
            Descriptive statistics include the numbers, tables, charts, and graphs used to describe, organize, summarize, and present raw data. Descriptive statistics are most often used to examine:<br>
            • <b>central tendency</b> (location) of data, i.e. where data tend to fall, as measured by the mean, median, and mode.<br>
            • <b>dispersion</b> (variability) of data, i.e. how spread out data are, as measured by the variance and its square root, the standard deviation.<br>
            • <b>skew</b> (symmetry) of data, i.e. how concentrated data are at the low or high end of the scale, as measured by the skew index.<br>
            • <b>kurtosis</b> (peakedness) of data, i.e. how concentrated data are around a single value, as measured by the kurtosis index.<br><br>
            The following definitions are vital in understanding descriptive statistics:<br><br>

            <b>Variables</b>:<br>
            - Quantities or qualities that may assume any one of a set of values. Variables may be classified as nominal, ordinal, or interval.<br>
            - <b>Nominal variables</b>: Use names, categories, or labels for qualitative values.<br>
            - <b>Ordinal variables</b>: Categorical variables where the order or rank of the categories is meaningful.<br>
            - <b>Interval variables</b>: Purely numeric variables where the difference between values is meaningful, allowing statements about extent or degree.<br><br>

            <b>Frequency Distributions</b>:<br>
            - Summarize and compress data by grouping them into classes and recording how many data points fall into each class. The frequency distribution is the foundation of descriptive statistics, necessary for various graphs and basic statistics.<br><br>

            <b>Measures of Central Tendency</b>:<br>
            - Indicate the middle and commonly occurring points in a data set.<br>
            - <b>Mean</b>: The average, the most common measure of central tendency.<br>
            - <b>Median</b>: The value in the middle of the data set when arranged in order of magnitude.<br>
            - <b>Mode</b>: The value occurring most often in the data, least commonly used in large data sets but important for describing multimodal data sets.<be>

            <b>Measures of Dispersion</b><br>
            Measures of Dispersion indicate how spread out the data are around the mean. Measures of dispersion are especially helpful when data are normally distributed, i.e. closely resemble the bell curve. The most common measures of dispersion follow.<br><br>

            <b>Variance</b> is expressed as the sum of the squares of the differences between each observation and the mean, which quantity is then divided by the sample size. For populations, it is designated by the square of the Greek letter sigma (σ). For samples, it is designated by the square of the letter s (s^2). Since this is a quadratic expression, i.e. a number raised to the second power, variance is the second moment of statistics.<br><br>

            Variance is used less frequently than <b>standard deviation</b> as a measure of dispersion. Variance can be used when we want to quickly compare the variability of two or more sets of interval data. In general, the higher the variance, the more spread out the data.<br><br>

            Standard deviation is expressed as the positive square root of the variance, i.e. σ for populations and s for samples. It is the average difference between observed values and the mean. The standard deviation is used when expressing dispersion in the same units as the original measurements. It is used more commonly than the variance in expressing the degree to which data are spread out.<br><br>

            <b>Coefficient of variation</b> measures relative dispersion by dividing the standard deviation by the mean and then multiplying by 100 to render a percent. This number is designated as V for populations and v for samples and describes the variance of two data sets better than the standard deviation.<br><br>

            For example, one data set has a standard deviation of 10 and a mean of 5. Thus, values vary by two times the mean. Another data set has the same standard deviation of 10 but a mean of 5,000. In this case, the variance and, hence, the standard deviation are insignificant.<br><br>

            <b>Range</b> measures the distance between the lowest and highest values in the data set and generally describes how spread out data are. For example, after an exam, an instructor may tell the class that the lowest score was 65 and the highest was 95. The range would then be 30. Note that a good approximation of the standard deviation can be obtained by dividing the range by 4.<br><br>

            <b>Percentiles</b> measure the percentage of data points which lie below a certain value when the values are ordered. For example, a student scores 1280 on the Scholastic Aptitude Test (SAT). Her scorecard informs her she is in the 90th percentile of students taking the exam. Thus, 90 percent of the students scored lower than she did.<br><br>

            <b>Quartiles</b> group observations such that 25 percent are arranged together according to their values. The top 25 percent of values are referred to as the upper quartile. The lowest 25 percent of the values are referred to as the lower quartile. Often the two quartiles on either side of the median are reported together as the interquartile range. Examining how data fall within quartile groups describes how deviant certain observations may be from others. <br><br> 
            <b>Measures of Skew</b><br>
            Measures of skew describe how concentrated data points are at the high or low end of the scale of measurement. Skew is designated by the symbols Sk for populations and sk for samples. Skew indicates the degree of symmetry in a data set. The more skewed the distribution, the higher the variability of the measures, and the higher the variability, the less reliable are the data.<br><br>

            Skew is calculated by either multiplying the difference between the mean and the median by three and then dividing by the standard deviation or by summing the cubes of the differences between each observation and the mean and then dividing by the cube of the standard deviation. Note that the use of cubic quantities helps explain why skew is called the third moment.<br><br>

            More conceptually, skew defines the relative positions of the mean, median, and mode. If a distribution is skewed to the right (positive skew), the mean lies to the right of both the mode (most frequent value and hump in the curve) and median (middle value). That is, mode > median > mean.<br><br>

            But, if the distribution is skewed left (negative skew), the mean lies to the left of the median and the mode. That is, mean < median < mode. In a perfect distribution, mean = median = mode, and skew is 0. The values of the equations noted above will indicate left skew with a negative number and right skew with a positive number. <br><br>
            <b>Measures of Kurtosis</b><br>
            Measures of kurtosis describe how concentrated data are around a single value, usually the mean. Thus, kurtosis assesses how peaked or flat is the data distribution. The more peaked or flat the distribution, the less normally distributed the data. And the less normal the distribution, the less reliable the data.<br><br>

            Kurtosis is designated by the letter K for populations and k for samples and is calculated by raising the sum of the squares of the differences between each observation and the mean to the fourth power and then dividing by the fourth power of the standard deviation. Note that the use of the fourth power explains why kurtosis is called the fourth moment.<br><br>

            Three degrees of kurtosis are noted:<br><br>

            <b>Mesokurtic</b> distributions are, like the normal bell curve, neither peaked nor flat.<br><br>

            <b>Platykurtic</b> distributions are flatter than the normal bell curve.<br><br>

            <b>Leptokurtic</b> distributions are more peaked than the normal bell curve.<br><br>

            The ideal value rendered by the equation for kurtosis is 3, the kurtosis of the normal bell curve. The higher the number above 3, the more leptokurtic (peaked) is the distribution. The lower the number below 3, the more platykurtic (flat) is the distribution. <br><br>
            <b>WHEN TO USE DESCRIPTIVE RESEARCH</b><br>
            Descriptive statistics are recommended when the objective is to describe and discuss a data set more generally and conveniently than would be possible using raw data alone. They are routinely used in reports which contain a significant amount of qualitative or quantitative data. Descriptive statistics help summarize and support assertions of fact.<br><br>
    
            Note that a thorough understanding of descriptive statistics is essential for the appropriate and effective use of all normative and cause-and-effect statistical techniques, including hypothesis testing, correlation, and regression analysis.<br><br>
    
            Unless descriptive statistics are fully grasped, data can be easily misunderstood and, thereby, misrepresented.<br><br>
    
            Skew and kurtosis should be examined any time you deal with interval data since they jointly help determine whether the variable underlying a frequency distribution is normally distributed.<br><br>
    
            Since normal distribution is a key assumption behind most statistical techniques, the skew and kurtosis of any interval data set must be analyzed. Data that show significant variation, skew, or kurtosis should not be used in making inferences, drawing conclusions, or espousing recommendations. <br><br>
            <b>HOW TO PREPARE DESCRIPTIVE RESEARCH</b><br>
            Since statistical analysis software and most spreadsheets generate all required descriptive statistics, computer applications offer the best means of preparing such information. Nonetheless, for reference purposes, formulas for the various measures follow in the same order in which previously discussed. Note that in all cases, the use of Greek or upper case Latin letters refer to population parameters and that the use of lower case Latin letters refers to sample statistics. <br><br>
        </p>
    </div>
    <div id = "P3ITEP203">
        <h1 style="text-align: center;"> <b>Part 3</b></h1>
        <p>
            <b>Statistical Method and Data Analysis</b><br><br>
            <b>PROBABILITY</b><br><br>
            •Modern probability developments are increasingly sophisticated mathematically. To utilize these, the practitioner needs a sound conceptual basis which, fortunately, can be attained at a moderate level of mathematical sophistication. There is need to develop a feel for the structure of the underlying mathematical model, for the role of various types of assumptions, and for the principal strategies of problem formulation and solution.<br><br>
            <b>THE CONCEPT OF PROBABILITY</b><br><br>
            •Probability implies 'likelihood' or 'chance'. When an event is certain to happen then the probability of occurrence of that event is 1 and when it is certain that the event cannot happen then the probability of that event is 0.<br>
            •Hence the value of probability ranges from 0 to 1. Probability has been defined in a varied manner by various schools of thought.<br><br>
            <b>CLASSICAL DEFINITION OF PROBABILITY</b><br><br>
            As the name suggests the classical approach to defining probability is the oldest approach. It states that if there are n exhaustive, mutually exclusive and equally likely cases out of which m cases are favorable to the happening of event A, then the probabilities of event A is defined as given by the following probability function:<br>
            P(A)=Number of favorable cases/Total number of equally likely cases=m/n<br>
            •Thus, to calculate the probability we need information on number of favorable cases and total number of equally likely cases. This can he explained using following example.<br><br>
            <b>Example</b><br><br>
            Problem Statement:<br>
            •A coin is tossed. What is the probability of getting a head?<br>
            Solution:<br>
            •Total number of equally likely outcomes 👎 = 2 (i.e. head or tail) Number of outcomes favorable to P(head)=1/2<br>
            head (m) = 1<br><br>
            <b>SAMPLE EVENTS AND SPACE</b><br><br>
            •There are lots of phenomena in nature, like tossing a coin or tossing a die, whose outcomes cannot be predicted with certainty in advance, but the set of all the possible outcomes is known. These are what we call random phenomena or random experiments. Probability theory is concerned with such random phenomena or random experiments.<br>
            •Consider a random experiment. The set of all the possible outcomes is called the sample space of the experiment and is usually denoted by S. Any subset E of the sample space S is called an event.<br>
            •Example 1. Tossing a coin. The sample space is S = {H, T}. E = {H} is an event.<br><br>
            <b>CONDITIONAL PROBABILITY AND INDEPENDENCE</b><br><br>
            •For an experiment we define an event to be any collection of possible outcomes. A simple event is an event that consists of exactly one outcome.<br>
            •"or" means the union (i.e. either can occur)<br>
            •"and" means intersection (i.e. both must occur)<br>
            •Two events are mutually exclusive if they cannot occur simultaneously. For a Venn diagram, we can tell that two events are mutually exclusive if their regions do not intersect<br>
            •Probability:<br>
            P(E)= number of simple events within E/total number of possible outcomes<br>
            We have the following:<br>
            1.P(E) is always between 0 and 1.<br>
            2.The sum of the probabilities of all simple events must be 1.<br>
            3.P(E)+P(not E)=1<br>
            4.If E and F are mutually exclusive then P(E or F)=P(E)+P(F)<br><br>
            <b>The Difference Between "and" and "or"</b><br><br>
            •If E and F are events then we use the terminology E and F(3) to mean all outcomes that belong to both E and F. We use the terminology E or F to mean all outcomes that belong to either E or F.<br>
            •Example:<br>
            • Our Women's Volleyball team is recruiting for new members. Suppose that a person inquires about the team.<br>
            • Let E be the event that the person is female.<br>
            • Let F be the event that the person is a student then E and F represents the qualifications for being a member of the team. Note that E or F is not enough.<br>
            • We define: P(E|F)= P(E and F)/P(F)<br>
            • Conditional Probability<br>
            • We read the left-hand side as "The probability of event E given event F occurred." We call two events independent if the following definitions hold.<br>
            • Independence<br>
            • For independent Events P(E|F)=P(E)<br>
            • Equivalently, we can say that E and F are independent if The Multiplication Rule<br>
            • For independent Events P(E and F)=P(E)P(F)<br>
            • Example:<br>
            • Consider rolling two dice. Let<br>
            • E be the event that the first die is a 3.<br>
            • F be the event that the sum of the dice is an 8.<br>
            • Then E and F means that we rolled a three and then we rolled a 5<br>
            • This probability is 1/36 since there are 36 possible pairs and only one of them is (3,5) We have<br>
            • P(E)=1/6<br>
            • And note that (2,6),(3,5),(4,4),(5,3), and (6,2) give F<br>
            • Hence<br>
            • P(F)=5/36<br>
            • We have<br>
            • P(E)P(F)=(1/6)(5/36)<br>
            • which is not 1/36 and we can conclude that E and F are not independent.<br><br>
            <b>Counting Rule</b><br><br>
            • For two events, E and F, we always have<br>
            • P(E or F)=P(E)+P(F)−P(E and F)<br>
            • Example:<br>
            • Find the probability of selecting either a heart or a face card from a 52-card deck.<br>
            Solution:<br>
            •We let<br>
            • E = the event that a heart is selected<br>
            • F = the event that a face card is selected then P(E)=1/4 and P(F)=3/13<br>
            • that is, Jack, Queen, or King out of 13 different cards of one kind.<br>
            • P(E and F)=3/52<br>
            • The counting rule formula (P(E or F)=P(E)+P(F)) gives P(E or F)=1/4+3/13−3/52=22/52=42%<br><br>
            <b>Sample Problem and Solution in Probability</b><br><br>
            •Example 1: A coin is thrown 3 times. What is the probability that at least one head is obtained?<br>
            •Solution: Sample space = [HHH, HHT, HTH, THH, TTH, THT, HTT, TTT]<br>
            •Total number of ways = 2 × 2 × 2 = 8. Fav. Cases = 7<br>
            •P (A) = 7/8<br>
            OR<br>
            •P (of getting at least one head) = 1 – P (no head)⇒<br>
            1 – (1/8) = 7/8<br><br>
            <b>Linear Programming</b><br><br>
            •Linear programming is a mathematical concept used to determine the solution to a linear problem. Typically, the goal of linear programming is to maximize or minimize specified objectives, such as profit or cost. This process is known as optimization. It relies upon three different concepts: variables, objectives, and constraints.<br>
            •Variables are numerical or Boolean values, such as quantity of product to be produced or whether a distribution center is open. Linear programming determines the optimal value for such variables. Objectives, such as maximum profit, are a linear function of an optimization’s variables. <br>
            •Constraints are also linear functions of an optimization’s variables and are to use to restrict the values an optimization can return for a variable. Constraints must be Boolean linear functions. Examples of constraints could be a specified ratio of budget allocation or the total number of items a factory can produce.<br>
            •Thinking about linear programming in terms of a chart can help you visualize and understand it. Linear functions such as objectives and constraints are displayed as straight lines on a graph. Variables can be thought of as x and y, or values that we don't yet know.<br>
            •The optimal value for an objective will often be in a corner of this feasible region, as this will be the maximal or minimal feasible value for the objective.<br>
            •Linear programming is a simple technique where we depict complex relationships through linear functions and then find the optimum points. Applications of linear programming are everywhere around you. You use linear programming at personal and professional fronts. You are using linear programming when you are driving from home to work and want to take the shortest route. Or when you have a project delivery you make strategies to make your team work efficiently for on-time delivery.<br><br>
            <b>Common Terminologies used in Linear Programming</b><br><br>
            •Let us define some terminologies used in Linear Programming using the above example.<br>
            • Decision Variables: The decision variables are the variables that will decide my output. They represent my ultimate solution. To solve any problem, we first need to identify the decision variables. For the above example, the total number of units for A and B denoted by X & Y respectively are my decision variables.<br>
            • Objective Function: It is defined as the objective of making decisions. In the above example, the company wishes to increase the total profit represented by Z. So, profit is my objective function.<br>
            •Constraints: The constraints are the restrictions or limitations on the decision variables. They usually limit the value of the decision variables. In the above example, the limit on the availability of resources Milk and Choco are my constraints.<br>
            • Non-negativity restriction: For all linear programs, the decision variables should always take non-negative values. This means the values for decision variables should be greater than or equal to 0.<br>
            The process to formulate a Linear Programming problem<br>
            •Let us look at the steps of defining a Linear Programming problem generically:<br>
            1.Identify the decision variables<br>
            2.Write the objective function<br>
            3.Mention the constraints<br>
            4.Explicitly state the non-negativity restriction<br>
            •For a problem to be a linear programming problem, the decision variables, objective function and constraints all have to be linear functions.<br>
            •If all the three conditions are satisfied, it is called a Linear Programming Problem.<br><br>
            <b>Solve Linear Programs by Graphical Method</b><br><br>
            A linear program can be solved by multiple methods. In this section, we are going to look at the Graphical method for solving a linear program. This method is used to solve a two-variable linear program. If you have only two decision variables, you should use the graphical method to find the optimal solution.<br>
            •A graphical method involves formulating a set of linear inequalities subject to the constraints. Then the inequalities are plotted on an X-Y plane. Once we have plotted all the inequalities on a graph the intersecting region gives us a feasible region. The feasible region explains what all values our model can take. And it also gives us the optimal solution.<br><br>
            <b>Regression Analysis</b><br><br>
            •Suppose you’re a sales manager trying to predict next month’s numbers. You know that dozens, perhaps even hundreds of factors from the weather to a competitor’s promotion to the rumor of a new and improved model can impact the number. <br>
            •Perhaps people in your organization even have a theory about what will have the biggest effect on sales. “Trust me. The more rain we have, the more we sell.” “Six weeks after the competitor’s promotion, sales jump.”<br>
            •Regression analysis is a way of mathematically sorting out which of those variables does indeed have an impact. It answers the questions: Which factors matter most? Which can we ignore? How do those factors interact with each other? And, perhaps most importantly, how certain are we about all of these factors?<br>
            •In regression analysis, those factors are called variables. You have your dependent variable — the main factor that you’re trying to understand or predict. In the example above, the dependent variable is monthly sales. And then you have your independent variables — the factors you suspect have an impact on your dependent variable.<br>
            <b>Regression Model:</b><br><br>
            A regression model determines a relationship between an independent variable and a dependent variable, providing a function. Formulating a regression analysis helps you predict the effects of the independent variable on the dependent one.

            Example: we can say that age and height can be described using a <b>linear regression model</b>. Since a person’s height increases as age increases, they have a linear relationship. Regression models are commonly used as statistical proof of claims regarding everyday facts.

            <b>Linear Regression Model:</b><br><br>
            A linear regression model is used to depict a relationship between variables that are proportional to each other. This means that the dependent variable increases/decreases with the independent variable.

            In the graphical representation, it has a straight linear line plotted between the variables. Even if the points are not exactly in a straight line (which is always the case) we can still see a pattern and make sense of it. For example, as the age of a person increases, the level of glucose in their body increases as well.

            <b>Example Computation of Linear Regression:</b><br><br>
            The regression equation is written as Y = a + bX + e where:<br><br>
            Y is the value of the <b>dependent variable</b> (Y), what is being predicted or explained.<br>
            a or Alpha, a constant; equals the value of Y when the value of X=0.<br>
            b or Beta, the coefficient of X; the slope of the regression line; how much Y changes for each one-unit change in X.<br>
            X is the value of the <b>independent variable</b> (X), what is predicting or explaining the value of Y.<br>
            e is the <b>error term</b>; the error in predicting the value of Y, given the value of X (it is not displayed in most regression equations).

            <b>Example 1:</b><br><br>
            For example, say we know what the average speed is of cars on the freeway when we have 2 highway patrols deployed (average speed=75 mph) or 10 highway patrols deployed (average speed=35 mph). But what will be the average speed of cars on the freeway when we deploy 5 highway patrols?<br><br>
            From our known data, we can use the regression formula to compute the values of a and b and obtain the following equation: Y = 85 + (-5)X, where:<br><br>
            Y is the average speed of cars on the freeway.<br>
            a = 85, or the average speed when X=0.<br>
            b = (-5), the impact on Y of each additional patrol car deployed.<br>
            X is the number of patrol cars deployed.<br><br>
            That is, the average speed of cars on the freeway when there are no highway patrols working (X=0) will be 85 mph. For each additional highway patrol car working, the average speed will drop by 5 mph.<br><br>
            For five patrols (X=5), Y = 85 + (-5)(5) = 85 - 25 = 60 mph - average speed of cars on the freeway when we deploy 5 highway patrols.

            <b>Example 2:</b><br><br>
            Number of questions correct out of 20 that 10 students taken on test (X)<br>
            Attitude in taking test<br>
            Number of correct (X) Attitude (Y)<br>
            17 96<br>
            13 73<br>
            12 59<br>
            15 80<br>
            16 93<br>
            14 85<br>
            16 66<br>
            16 79<br>
            18 77<br>
            19 91<br><br>
            <b>Linear Regression Function:</b><br><br>
            y = a + bx<br><br>
            <b>Slope (b) of regression line:</b><br><br>
            b = r(Sy/Sx)<br><br>
            r is Pearson’s correlation coefficient.<br>
            Sy is the standard deviation of y.<br>
            Sx is the standard deviation of x.<br><br>
            <b>y-intercept of regression line:</b><br><br>
            a = y - bx<br><br>
            <b>To get the Pearson’s correlation coefficient:</b><br><br>
            x y (x-¯) (y-ӯ) (x-¯x)* (y-ӯ) (x-¯x) 2 (y-ӯ) 2<br><br>
            <b>Pearson Correlation Coefficient:</b><br><br>
            r = ∑((x-x)(y-y)) / √∑(x-x) 2 ∑(y-y) 2<br><br>
            r = 137.6/√42.4*1266.9 = 0.593692022<br><br>
            Sy = √∑(y-y) 2 / n-1<br><br>
            Sy = √1266.9 / 9 = 11.865<br><br>
            Sx = √∑(x-x) 2 / n-1<br><br>
            Sx = √42.4 / 9 = 2.171<br><br>
            b = r(Sy / Sy)<br><br>
            b = 0.593(11.865/2.171) = 3.241<br><br>
            a = y - b x<br><br>
            a = 79.9 - 3.241(15.6) = 29.34<br><br>
            y = a + bx<br><br>
            y = 29.34 + 3.24 x<br><br>
            y = 29.34 + 3.24 (15) **supposed the value of x is 15<br><br>
            y = 77.94
        </p>
    </div>
    <div id = "P4ITEP203">
        <h1 style="text-align: center;"> <b>Part 4</b></h1>
        <p>
            <b>Measures of Central Tendency</b><br>
            <b>Mean</b><br>
            Note that F = 1 for raw data since each datum is a class unto itself.<br>
            <br>
            <b>Mean Example:</b><br>
            The mean of 4, 1, and 7 is (4 + 1 + 7) / 3 = 12 / 3 = 4<br>
            <br>
            <b>Mean in Excel</b><br>
            Arithmetic mean, also referred to as average, is probably the measure you are most familiar with. The mean is calculated by adding up a group of numbers and then dividing the sum by the count of those numbers.<br>
            For example, to calculate the mean of numbers {1, 2, 2, 3, 4, 6}, you add them up, and then divide the sum by 6, which yields 3: (1 + 2 + 2 + 3 + 4 + 6) / 6 = 3.<br>
            In Microsoft Excel, the mean can be calculated by using one of the following functions:<br>
            <br>
            AVERAGE - returns an average of numbers.<br>
            AVERAGEA - returns an average of cells with any data (numbers, Boolean and text values).<br>
            AVERAGEIF - finds an average of numbers based on a single criterion.<br>
            AVERAGEIFS - finds an average of numbers based on multiple criteria.<br>
            =AVERAGE(C2:C8)<br>
            To get the average of only "Banana" sales, use an AVERAGEIF formula:<br>
            =AVERAGEIF(A2:A8, "Banana", C2:C8)<br>
            To calculate the mean based on 2 conditions, say, the average of "Banana" sales with the status "Delivered", use AVERAGEIFS: =AVERAGEIFS(C2:C8, A2:A8, "Banana", B2:B8, "Delivered")<br>
            <br>
            <b>Median</b><br>
            Median Example<br>
            The median of 4, 1, and 7 is 4 because when the numbers are put in order (1, 4, 7), the number 4 is in the middle.<br>
            <br>
            <b>Median in Excel</b><br>
            Median is the middle value in a group of numbers, which are arranged in ascending or descending order, i.e. half the numbers are greater than the median and half the numbers are less than the median. For example, the median of the data set {1, 2, 2, 3, 4, 6, 9} is 3.<br>
            This works fine when there are an odd number of values in the group. But what if you have an even number of values? In this case, the median is the arithmetic mean (average) of the two middle values. For example, the median of {1, 2, 2, 3, 4, 6} is 2.5. To calculate it, you take the 3rd and 4th values in the data set and average them to get a median of 2.5.<br>
            In Microsoft Excel, a median is calculated by using the MEDIAN function. For example, to get the median of all amounts in our sales report, use this formula:<br>
            =MEDIAN(C2:C8)<br>
            <br>
            <b>Mode</b><br>
            Mode Example<br>
            The mode of {4, 2, 4, 3, 2, 2} is 2 because it occurs three times which is more than any other number.<br>
            <br>
            <b>Mode in Excel</b><br>
            Mode is the most frequently occurring value in the dataset. While the mean and median require some calculations, a mode value can be found simply by counting the number of times each value occurs.<br>
            For example, the mode of the set of values {1, 2, 2, 3, 4, 6} is 2. In Microsoft Excel, you can calculate a mode by using the function of the same name, the MODE function. For our sample data set, the formula goes as follows:<br>
            =MODE(C2:C8)<br>
            <br>
            <b>Measures of Dispersion</b><br>
            <b>Variance</b><br>
            <b>Standard deviation</b><br>
            <b>Variance and Standard deviation example:</b><br>
            You and your friends have just measured the heights of your dogs (in millimeters):<br>
            The heights (at the shoulders) are: 600mm, 470mm, 170mm, 430mm, and 300mm. Find out the Mean, the Variance, and the Standard Deviation.<br>
            Your first step is to find the Mean:<br>
            So the mean (average) height is 394 mm. Let's plot this on the chart:<br>
            Now we calculate each dog's difference from the Mean:<br>
            To calculate the Variance, take each difference, square it, and then average the result:<br>
            So the Variance is 21,704<br>
            And the Standard Deviation is just the square root of Variance,<br>
            And the good thing about the Standard Deviation is that it is useful. Now we can show which heights are within one Standard Deviation (147mm) of the Mean:<br>
            So, using the Standard Deviation, we have a "standard" way of knowing what is normal, and what is extra-large or extra small.<br>
            <b>Variance in Excel</b><br>
            Calculate the mean (simple average) of the five numbers:<br>
            From each number, subtract the mean to find the differences.<br>
            Square each difference.<br>
            Work out the average of the squared differences.<br>
            <b>Standard Deviation in Excel</b><br>
            For example, the numbers below have a mean (average) of 10.<br>
            Explanation: the numbers are all the same which means there's no variation. As a result, the numbers have a standard deviation of zero. The STDEV function is an old function. Microsoft Excel recommends using the new STEDV.S function which produces the exact same result.<br>
            <b>Coefficient of Variation</b><br>
            <b>Coefficient of Variation Example</b><br>
            Two versions of a test are given to students. One test has pre-set answers and a second test has randomized answers. Find the coefficient of variation.<br>
            Step 1: Divide the standard deviation by the mean for the first sample:<br>
            11.2 / 50.1 = 0.22355<br>
            Step 2: Multiply Step 1 by 100:<br>
            0.22355 * 100 = 22.355%<br>
            Step 3: Divide the standard deviation by the mean for the second sample:<br>
            12.9 / 45.8 = 0.28166<br>
            Step 4: Multiply Step 3 by 100:<br>
            0.28166 * 100 = 28.266%<br>
            <b>Coefficient of Variation in Excel</b><br>
            You can calculate the coefficient of variation in Excel using the formulas for standard deviation and mean. For a given column of data (i.e. A1:A10), you could enter:<br>
            “=stdev(A1:A10)/average(A1:A10)) then multiply by 100.<br>
            <b>Range</b><br>
            Range = Largest Value - Smallest Value<br>
            An approximation of the standard deviation is the range divided by 4.<br>
            <b>Range Example</b><br>
            The difference between the lowest and highest values.<br>
            In {4, 6, 9, 3, 7} the lowest value is 3, and the highest is 9, so the range is 9 − 3 = 6.<br>
            <b>Range in Excel</b><br>
            You can also use the formulas above for minimum and maximum to calculate range using Excel in a single step. Imagine you have data running from cell A2 to cell A20. Type<br>
            "=MAX(A2:A20)-MIN(A2:A20)"<br>
            to find the range in a single step. This tells Excel to find the maximum of the data and then subtract the minimum of the data from it.<br>
            <b>Percentiles</b><br>
            Populations or samples:<br>
            The pth percentile is the value for which at most p% of the values are than that value and at most (100 - p%) of the values are greater than that value. The median is the 50th percentile.<br>
            Example question:<br>
            Score is 30, 33, 43, 53, 56, 67, 68, 72<br>
            Rank is 1, 2, 3, 4, 5, 6, 7, 8<br>
            Find out where the 25th percentile is in the above list.<br>
            Step 1: Calculate what rank is at the 25th percentile. Use the following formula:<br>
            Rank = Percentile / 100 * (number of items + 1) Rank = 25 / 100 * (8 + 1) = 0.25 * 9 = 2.25.<br>
            A rank of 2.25 is at the 25th percentile. However, there isn’t a rank of 2.25 (ever heard of a high school rank of 2.25? I haven’t!), so you must either round up, or round down. As 2.25 is closer to 2 than 3, I’m going to round down to a rank of 2.
            <b>Step 2: Choose either definition 1 or 2:</b>

            Definition 1: The lowest score that is greater than 25% of the scores. That equals a score of 43 on this list (a rank of 3).<br><br>

            Definition 2: The smallest score that is greater than or equal to 25% of the scores. That equals a score of 33 on this list (a rank of 2).<br><br>
            
            Depending on which definition you use, the 25th percentile could be reported at 33 or 43! A third definition attempts to correct this possible misinterpretation:<br><br>
            
            Definition 3: A weighted mean of the percentiles from the first two definitions.<br><br>
            
            In the example, here’s how the percentile would be worked out using the weighted mean:<br><br>
            
            Multiply the difference between the scores by 0.25 (the fraction of the rank we calculated above). The scores were 43 and 33, giving us a difference of 10:<br>
            (0.25)(43 – 33) = 2.5<br><br>
            
            Add the result to the lower score. 2.5 + 33 = 35.5<br><br>
            
            In this case, the 25th percentile score is 35.5, which makes more sense as it’s in the middle of 43 and 33.<br><br>
            
            In most cases, the percentile is usually definition #1. However, it would be wise to double check that any statistics about percentiles are created using that first definition.<br><br>
            
            <b>Percentiles in Excel:</b><br><br>
            
            The Excel PERCENTILE function calculates the "kth percentile" for a set of data. A percentile is a value below which a given percentage of values in a data set fall. You can use PERCENTILE to determine the 90th percentile, the 80th percentile, etc.<br><br>
            
            <b>-Purpose:</b><br>
            Get kth percentile<br><br>
            
            <b>-Return value:</b><br>
            Calculated percentile for k<br><br>
            
            <b>-Syntax:</b><br>
            =PERCENTILE(array, k)<br><br>
            
            <b>-Arguments:</b><br>
            array - Data values.<br>
            k - Number representing kth percentile.<br><br>
            
            <b>-Quartiles:</b><br><br>
            
            Populations or samples:<br><br>
            
            First quartile = Q1 = 25th percentile<br>
            Second quartile = Q2 = 50th percentile (median)<br>
            Third quartile = Q3 = 75th percentile<br><br>
            
            <b>Quartiles Example:</b><br><br>
            
            -Divide the following data set into quartiles: 2, 5, 6, 7, 10, 22, 13, 14, 16, 65, 45, 12.<br><br>
            
            -Step 1: Put the numbers in order: 2, 5, 6, 7, 10, 12, 13, 14, 16, 22, 45, 65.<br>
            -Step 2: Count how many numbers there are in your set and then divide by 4 to cut the list of numbers into quarters. There are 12 numbers in this set, so you would have 3 numbers in each quartile.<br>
            2, 5, 6, | 7, 10, 12 | 13, 14, 16, | 22, 45, 65<br><br>
            
            The calculator gives you the 25th Percentile, which is the end of the first quartile, the 50th Percentile which is the end of the second quartile (or the median) and the 75th Percentile, which is the end of the third quartile. For 10, 13, 17, 19 and 21 the results are:<br>
            25th Percentile: 11.5<br>
            50th Percentile: 17<br>
            75th Percentile: 20<br>
            Interquartile Range: 8.5.<br><br>
            
            <b>Quartiles in Excel:</b><br><br>
            
            The Excel QUARTILE function returns the quartile (each of four equal groups) for a given set of data. QUARTILE can return minimum value, first quartile, second quartile, third quartile, and max value.<br><br>
            
            <b>-Purpose:</b><br>
            Get the quartile in a data set<br><br>
            
            <b>-Return value:</b><br>
            Value for requested quartile<br><br>
            
            <b>-Syntax:</b><br>
            =QUARTILE(array, quart)<br><br>
            
            <b>-Arguments:</b><br>
            array - A reference containing data to analyze.<br>
            quart - The quartile value to return.<br><br>
            
            <b>Measures of Skew</b><br><br>
            
            Formulas for assessing the skew of a data set follow below. The ideal value of these equations is 0, the skew of the perfectly distributed normal bell curve.<br><br>
            
            <b>Skew</b><br><br>
            
            Positive (Right) Skewness Example:<br><br>
            
            A scientist has 1,000 people complete some psychological tests. For test 5, the test scores have skewness = 2.0.<br><br>
            
            The histogram shows a very asymmetrical frequency distribution. Most people score 20 points or lower but the right tail stretches out to 90 or so. This distribution is right skewed.<br><br>
            
            If we move to the right along the x-axis, we go from 0 to 20 to 40 points and so on. So towards the right of the graph, the scores become more positive. Therefore,<br>
            right skewness is positive skewness<br>
            which means skewness > 0. This first example has skewness = 2.0 as indicated in the right top corner of the graph. The scores are strongly positively skewed.<br><br>
            
            Negative (Left) Skewness Example:<br><br>
            
            Another variable -the scores on test 2- turn out to have skewness = -1.0. Their histogram is shown below.<br><br>
            
            The bulk of scores are between 60 and 100 or so. However, the left tail is stretched out somewhat. So this distribution is left skewed.<br><br>
            
            Right: to the left, to the left. If we follow the x-axis to the left, we move towards more negative scores. This is why<br>
            left skewness is negative skewness<br>
            And indeed, skewness = -1.0 for these scores. Their distribution is left skewed. However, it is less skewed -or more symmetrical- than our first example which had skewness = 2.0.<br><br>
            
            Symmetrical Distribution Implies Zero Skewness:<br><br>
            
            Finally, symmetrical distributions have skewness = 0. The scores on test 3, having skewness = 0.1, come close.<br><br>
            
            Now, observed distributions are rarely precisely symmetrical. This is mostly seen for some theoretical sampling distributions. Some examples are<br>
            - the (standard) normal distribution;<br>
            - the t distribution; and<br>
            - the binomial distribution if p = 0.5.<br><br>
            
            These distributions are all exactly symmetrical and thus have skewness = 0.000...<br><br>
            
            An easier option for obtaining sample skewness is using<br>
            =SKEW(...).<br>
            which confirms the outcome of our manual calculation.<br><br>
            
            <b>Measures of Kurtosis</b><br><br>
            
            Formulas for assessing the kurtosis of a data set follow below. The ideal value of these equations is 3, the kurtosis of the perfectively distributed normal bell curve. The higher the value above 3, the more peaked is distribution. The lower the value below 3, the more flat is distribution.<br><br>
            
            <b>Kurtosis</b><br><br>
            
            Kurtosis Example:<br><br>
            
            Sample kurtosis is always measured relative to the kurtosis of a normal distribution, which is 3. Therefore, we are always interested in the “excess“ kurtosis, i.e., Excess kurtosis = sample kurtosis – 3, where:<br><br>
            
            Interpretation: A positive excess kurtosis indicates a leptokurtic distribution. A zero value indicates a mesokurtic distribution. Lastly, a negative excess kurtosis represents a platykurtic distribution.<br><br>
            
            Suppose we have the following observations: {12 13 54 56 25}<br><br>
            
            Determine the skewness of the data.<br><br>
            
            Solution:<br><br>
            
            First, we must determine the sample mean and the sample standard deviation:<br><br>
            
            Therefore,<br><br>
            
            Now we can work out the skewness:<br><br>
            
            Skewness is positive. Hence, the data has a positively skewed distribution.<br><br>
            
            <b>Kurtosis in Excel:</b><br><br>
            
            With Excel it is very straightforward to calculate kurtosis. Performing the following steps streamlines the process of using the formula displayed above. Excel's kurtosis function calculates excess kurtosis.<br><br>
            
            - Enter the data values into cells.<br>
            - In a new cell type =KURT(<br>
            - Highlight the cells where the data are at. Or type the range of cells containing the data.<br>
            - Make sure to close the parentheses by typing )<br>
            - Then press the enter key.<br><br>
            
            The value in the cell is the excess kurtosis of the data set.<br><br>
            
            For smaller data sets, there is an alternate strategy that will work: In an empty cell type =KURT(<br>
            - Enter the data values, each separated by a comma.<br>
            - Close the parentheses with )<br>
            - Press the enter key.<br><br>
            
            This method is not as preferable because the data are hidden within the function, and we cannot do other calculations, such as a standard deviation or mean, with the data that we have entered.<br><br>
            
            <b>Limitations</b><br><br>
            
            It is also important to note that Excel is limited by the amount of data that the kurtosis function, KURT, can handle. The maximum number of data values that can be used with this function is 255.<br><br>
            
            Due to the fact that the function contains the quantities (n - 1), (n - 2), and (n - 3) in the denominator of a fraction, we must have a data set of at least four values in order to use this Excel function. For data sets of size 1, 2, or 3, we would have a division by zero error. We also must have a nonzero standard deviation in order to avoid a division by zero error.<br><br>
        </p>
    </div>
    <div id = "P5ITEP203">
        <h1 style="text-align: center;"> <b>Part 5</b></h1>
        <p>
            <b>DISCRETE SYSTEM SIMULATION</b><br><br>
            In discrete systems, the changes in the system state are discontinuous and each change in the state of the system is called an event. The model used in a discrete system simulation has a set of numbers to represent the state of the system, called as a state descriptor.
            <br>
            <b>Discrete Event Simulation ─ Key Features</b><br><br>
            • Discrete event simulation is generally carried out by a software designed in high-level programming languages such as Pascal, C++, or any specialized simulation language. Following are the five key features:<br>
            • <b>Entities</b> − These are the representation of real elements like the parts of machines.<br>
            • <b>Relationships</b> − It means to link entities together.<br>
            • <b>Simulation Executive</b> − It is responsible for controlling the advance time and executing discrete events.<br>
            • <b>Random Number Generator</b> − It helps to simulate different data coming into the simulation model.<br>
            • <b>Results & Statistics</b> − It validates the model and provides its performance measures. <br>

            <b>Steps in a Simulation</b><br><br>

            1. <b>Problem Formulation</b><br>
            • Every study should begin with a statement of the problem.<br>
            2. <b>Setting of objectives and overall project plan</b><br>
            • The objectives indicate the questions to be answered by simulation.<br>
            3. <b>Model conceptualization</b><br>
            • The construction of a model of a system is probably as much art as science.<br>
            4. <b>Data collection</b><br>
            • There is a constant interplay between the construction of the model and the collection of the needed input data.<br>
            5. <b>Model translation</b><br>
            • Most real-world systems result in models that require a great deal of information storage and computation.<br>
            6. <b>Verified?</b><br>
            • Verification pertains to the computer program that has been prepared for the simulation model.<br>
            7. <b>Validated?</b><br>
            • Validation usually is achieved through the calibration of the model.<br>
            8. <b>Experimental design</b><br>
            • The alternatives that are to be simulated must be determined.<br>
            9. <b>Production runs and analysis</b><br>
            • Production runs and their subsequent analysis are used to estimate measures of performance for the system designs that are being simulated.<br>
            10. <b>More runs?</b><br>
            • Given the analysis of runs that have been completed, the analyst determines whether additional runs are needed and what design those additional experiments should follow.<br>
            11. <b>Documentation and reporting</b><br>
            • There are two types of documentation: program and progress.<br>
            12. <b>Implementation</b><br>
            • The success of the implementation phase depends on how well the previous eleven steps have been performed.
            <br><br>
            <b>Time Graph Representation</b><br>
            • Every system depends on a time parameter. In a graphical representation, it is referred to as clock time or time counter and initially it is set to zero.<br>
            • Time is updated based on the following two factors:<br>
            • <b>Time Slicing</b> − It is the time defined by a model for each event until the absence of any event.<br>
            • <b>Next Event</b> − It is the event defined by the model for the next event to be executed instead of a time interval. It is more efficient than Time Slicing.
            <br><br>
            <b>Simulation of a Queuing System</b><br>
            • A queue is the combination of all entities in the system being served and those waiting for their turn.<br>
            • Parameters Single Server Queue<br>
            • This is the simplest queuing system as represented in the following figure.<br>
            • The central element of the system is a server, which provides service to the connected devices or items.<br>
            • Items request to the system to be served, if the server is idle.<br>
            • Then, it is served immediately, else it joins a waiting queue.<br>
            • After the task is completed by the server, the item departs.
            <br><br>
            <b>Simulation of Time-Sharing System</b><br>
            • Time-sharing system is designed in such a manner that each user uses a small portion of time shared on a system, which results in multiple users sharing the system simultaneously.<br>
            • The switching of each user is so rapid that each user feels like using their own system.<br>
            • It is based on the concept of CPU scheduling and multi-programming where multiple resources can be utilized effectively by executing multiple jobs simultaneously on a system.<br>
            • Example − SimOS Simulation System.<br>
            • It is designed by Stanford University to study the complex computer hardware designs, to analyze application performance, and to study the operating systems. SimOS contains software simulation of all the hardware components of the modern computer systems, i.e. processors, Memory Management Units (MMU), caches, etc.
            <br><br>
            <b>CONTINUOUS SYSTEM SIMULATION</b><br>
            • A continuous system is one in which important activities of the system complete smoothly without any delay, i.e. no queue of events, no sorting of time simulation, etc. When a continuous system is modeled mathematically, its variables representing the attributes are controlled by continuous functions.
            <br><br>
            <b>What is Continuous Simulation?</b><br>
            • Continuous simulation is a type of simulation in which state variables change continuously with respect to time.
            <br><br>
            <b>Why Use Continuous Simulation?</b><br>
            • We have to use continuous simulation as it depends on differential equation of various parameters associated with the system and their estimated results known to us.
            <br><br>
            <b>Application Areas</b><br>
            • Continuous simulation is used in the following sectors:<br>
            • In civil engineering for the construction of dam embankment and tunnel constructions.<br>
            • In military applications for simulation of missile trajectory, simulation of fighter aircraft training, and designing & testing of intelligent controller for underwater vehicles.<br>
            • In logistics for designing of toll plaza, passenger flow analysis at the airport terminal, and proactive flight schedule evaluation.<br>
            • In business development for product development planning, staff management planning, and market study analysis.
        </p>
    </div>
    <div id = "P6ITEP203">
        <h1 style="text-align: center;"> <b>Part 6</b></h1>
        <p>
            <b>MONTE-CARLO SIMULATION</b><br>
            The term Monte Carlo is typically associated with the process of modeling and simulating a system affected by randomness: Several random scenarios are generated, and relevant statistics are gathered in order to assess, e.g., the performance of a decision policy or the value of an asset. <br><br>
            Monte Carlo methods are extremely flexible and valuable tools, quite often the last resort approach for overly complicated problems impervious to a more mathematically elegant treatment. It is also true that running a bad Monte Carlo simulation is very easy as well. <br><br>
            There are several reasons why this may happen: <br>

            We are using a wrong model of uncertainty: <br>
            Because we are using an unrealistic probability distribution <br>
            Or because we are missing some link among the underlying risk factors <br>
            Or because some unknown parameters have been poorly estimated <br>
            Or because the very nature of uncertainty in our problem does not lend itself to a stochastic representation <br>
            The output estimates are not reliable enough, i.e., the estimator variance is so large that a much larger sample size is required. <br>
            There is a systematic error in the estimates, which could be low or high biased. <br>
            The way we generate sample paths, possibly discretizing a continuous time model, induces a non-negligible error. <br>
            We are using poor random variate generators. <br>
            There is some possibly subtle bug in the computer program implementing the method. <br><br>

            <b>HISTORICAL ORIGIN OF MONTE CARLO SIMULATION</b><br>
            Monte Carlo methods involve random sampling, but the actual aim is to estimate a deterministic quantity. Indeed, a well-known and early use of Monte Carlo-like methods is Buffon's needle approach to estimate. <br>
            A huge sample size is needed to obtain a fairly accurate estimate, and this is certainly not the smartest way to estimate. <br>
            In more recent times, in the period between 1930 and 1950, Monte Carlo methods were used by the likes of Enrico Fermi, Stanislaw Ulam, and John von Neumann to solve problems related to physics. <br>
            Indeed, such methods were used in the 1950s when the hydrogen bomb was developed at the Los Alamos laboratories. It was then that the term Monte Carlo was coined after the well-known gambling house. Then, luckily enough, the idea moved to other domains, including operations research and economics. At present, Monte Carlo methods are extremely widespread in many domains, including risk management and financial engineering. <br><br>

            <b>MONTE CARLO SIMULATION VS. MONTE CARLO SAMPLING</b><br>
            The terms simulation and sampling are both used with reference to Monte Carlo methods. <br><br>

            <b>SYSTEM DYNAMICS AND THE MECHANICS OF MONTE CARLO SIMULATION</b><br>
            When we have to describe the dynamic evolution of a system over time, we typically resort to one of the following model classes: <br>

            Discrete-time models <br>
            Continuous-time models <br>
            Discrete-event models <br>
            For instance, even if we choose a continuous-time model, we have to discretize it in some way for the sake of computational feasibility. This implies that we actually simulate a discrete-time approximation, but given the issues involved in sample path generation, which involves nontrivial concepts related to the numerical solution of stochastic differential equations. It is also possible to create somewhat hybrid models, such as stochastic processes including both a diffusion component, which is continuous, and jumps, which are related to discrete-event dynamics. <br><br>
            <b>DISCRETE TIME MODELS</b><br>
            In discrete-time models we assume that the time horizon [0, T] is discretized in time intervals (time buckets) of width δt. In principle, nothing forbids nonuniform time steps, which may actually be required when dealing with certain financial derivatives; nevertheless, we usually stick to the uniform case for the sake of simplicity. The system is actually observed only at time instants of the form kδt, k-0,1,...,M, where T = M δt. What happens between two discrete-time instants is not relevant. Typically, we forget about the time bucket size δt, which could be a day, a week, or a month, and we use discrete-time subscripts, like t = 0,1,2,... , directly. <br><br>

            <b>R CODE TO SIMULATE A SIMPLE AUTOREGRESSIVE PROCESS</b><br>
            R programming notes: <br>

            In order to allocate memory to an array, we use rep (0, T+l) here; as an alternative, we may also use numeric (T+l).
            The syntax of for loops is self-explanatory. Braces are used to enclose multiple statements; in this case, since there is only one statement inside the loop, they could be omitted.
            The function plot is used here to lay down dots, with a character specified by the optional pch parameter; the function lines connects the dots with lines, without opening another window. <br>
            x[t] is a vector of control variables, i.e., decisions made after observing the state st.
            €t+1 is a vector of disturbances, realized after we apply the control variable xt
            If the sequence of disturbances €t is intertemporally independent, we obtain a Markov process. Markov processes are relatively easy to represent, simulate, and control. We should also observe that the state need not be a continuous variable. We may have a system with discrete states, resulting in a discrete-time Markov chain. As an example, we may consider a Markov chain modeling the migration in the credit rating of a bond issue. Sometimes, we use a discrete-state Markov chain to approximate a continuous- state system by aggregation: We partition the state space with a mutually exclusive and collectively exhaustive collection of subsets and identify each one with a single representative value. <br><br>

            <b>CONTINUOUS-TIME MODEL</b><br>
            The typical representation of a continuous-time model relies on differential equations. We are all familiar with the differential equation F = ma from elementary physics, where F is force, m is mass, and a is acceleration, i.e., the second-order derivative of position with respect to time. <br><br>

            <b>MONTE CARLO SIMULATION IMPLEMENTATION USING MS EXCEL</b><br>
            The Monte Carlo method supports a "stochastic" or "probabilistic" system, "a purely stochastic system is one whose state is randomly determined, having a random probability distribution or pattern that may be analyzed statistically but may not be predicted precisely."
        </p>
    </div>
    <script src="Navigation.js"></script>
</div>
</body>
</body>
</html>